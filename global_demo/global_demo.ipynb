{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of core features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports and some plot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio.v2 as iio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from numpy.random import Generator\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from genexpy import lower_bounds as gu\n",
    "from genexpy import kernels as ku\n",
    "from genexpy import probability_distributions as prob\n",
    "from genexpy import rankings_utils as ru\n",
    "from genexpy import mmd as mmd\n",
    "\n",
    "mpl.use(\"TkAgg\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"notebook\")\n",
    "palette = \"flare_r\"\n",
    "sns.set_palette(\"flare_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the hyperparameters from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "FIGURES_DIR = Path(\"figures_global\")\n",
    "\n",
    "FORMAT = config['format']['output']\n",
    "\n",
    "SEED = config['parameters']['seed']\n",
    "RNG = np.random.default_rng(SEED)\n",
    "EPS = config['parameters']['epsilon']\n",
    "ALPHA = config['parameters']['alpha']\n",
    "LR_CONFIDENCE = config['parameters']['lr_confidence']\n",
    "CI_LOWER = (1 - LR_CONFIDENCE) / 2\n",
    "CI_UPPER = LR_CONFIDENCE + CI_LOWER\n",
    "\n",
    "DATASET = Path(config['data']['dataset_path'])\n",
    "EXPERIMENTAL_FACTORS = config['data']['experimental_factors']\n",
    "TARGET = config['data']['target']\n",
    "ALTERNATIVES = config['data']['alternatives']\n",
    "\n",
    "SAMPLE_SIZE = config['sampling']['sample_size']\n",
    "DISJOINT = config['sampling']['disjoint']\n",
    "REPLACE = config['sampling']['replace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the dataset into a pandas dataframe and we check that only one experimental factor is set to `None`, indicating that it is allowed to vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET.suffix == '.parquet':\n",
    "    df = pd.read_parquet(DATASET).drop(columns=[\"time\"])\n",
    "elif DATASET.suffix == '.csv':\n",
    "    df = pd.read_csv(DATASET).drop(columns=[\"time\"])\n",
    "else:\n",
    "    raise Exception(\"Please use a Parquet or CSV file as the format of your data\")\n",
    "\n",
    "# Check whether exactly one of the experimental factors is None \n",
    "assert sum(value is None for value in EXPERIMENTAL_FACTORS.values()) == 1, \"Exactly one experimental factor must be None\"\n",
    "\n",
    "columns_to_check = set(EXPERIMENTAL_FACTORS.keys()).union({TARGET, ALTERNATIVES})\n",
    "# Check whether the factors listed in the config actually exist in the df\n",
    "if not_in_df := columns_to_check - set(df.columns):\n",
    "    raise ValueError(f\"The following columns are missing from the dataframe: {not_in_df}\")\n",
    "\n",
    "# Check whether the factors listed in the config are exhaustive\n",
    "if not_in_config:= set(df.columns) - columns_to_check:\n",
    "    raise ValueError(f\"The following columns in the dataframe are not required: {not_in_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a query from the experimental factors and convert the `df` to a rank matrix. Note that here, we assume that we are using the _all key to designate that we want to use all possible values for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factor_key = next((k for k, v in EXPERIMENTAL_FACTORS.items() if v == \"_all\"), None)\n",
    "dataframes_list = []\n",
    "\n",
    "if all_factor_key:\n",
    "    unique_values = df[all_factor_key].unique()\n",
    "    for value in unique_values:\n",
    "        current_factors = EXPERIMENTAL_FACTORS.copy()\n",
    "        current_factors[all_factor_key] = value\n",
    "        query_string = \" and \".join(f\"{key} == '{val}'\" if isinstance(val, str) else f\"{key} == {val}\"\n",
    "                                    for key, val in current_factors.items() if val is not None)\n",
    "        subset_df = df.query(query_string).reset_index(drop=True)\n",
    "        dataframes_list.append(subset_df)\n",
    "else:\n",
    "    query_string = \" and \".join(f\"{key} == '{val}'\" if isinstance(val, str) else f\"{key} == {val}\"\n",
    "                                for key, val in EXPERIMENTAL_FACTORS.items() if val is not None)\n",
    "    single_df = df.query(query_string).reset_index(drop=True)\n",
    "    dataframes_list.append(single_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the kernels that we defined in the config. We will later iterate over them, as to perform our analysis over all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kernels(rank_matrix):\n",
    "    kernels = {}\n",
    "    for kernel_config in config['kernels']:\n",
    "        kernel_func = getattr(ku, kernel_config['kernel'], None)\n",
    "        \n",
    "        if kernel_func:\n",
    "            for param_key, param_values in kernel_config['params'].items():\n",
    "                if isinstance(param_values, list):\n",
    "                    for value in param_values:\n",
    "                        params = {param_key: value}\n",
    "                        if param_key == 'idx':\n",
    "                            params[param_key] = rank_matrix.index.get_loc(value)\n",
    "                        \n",
    "                        kernel_name = f\"{kernel_config['kernel']}_{param_key}_{value}\"\n",
    "                        kernels[kernel_name] = (kernel_func, params)\n",
    "                else:\n",
    "                    params = {param_key: param_values}\n",
    "                    if param_key == 'idx':\n",
    "                        params[param_key] = rank_matrix.index.get_loc(param_values)\n",
    "\n",
    "                    kernel_name = f\"{kernel_config['kernel']}_{param_key}_{param_values}\"\n",
    "                    kernels[kernel_name] = (kernel_func, params)\n",
    "        else:\n",
    "            print(f\"Kernel function '{kernel_config['kernel']}' not found in module 'kernels'.\")\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some directories for the different experiments we run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_directory(name, factors):\n",
    "    exp0_dir_name_parts = [f\"name={ALTERNATIVES}\"] + [f\"{key}={value}\" for key, value in factors.items() if value is not None]\n",
    "    \n",
    "    exp0_dir = FIGURES_DIR / \"_\".join(exp0_dir_name_parts)\n",
    "    \n",
    "    exp1_dir = exp0_dir / f\"{name}\"\n",
    "    \n",
    "    exp2_dir = exp1_dir / f\"nstar_N_ALPHA={ALPHA}_eps={EPS}_ci={LR_CONFIDENCE}_disjoint={DISJOINT}_replace={REPLACE}\"\n",
    "    \n",
    "    exp2_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return exp2_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a few methods that will be run in a loop until our `ec_pool` is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by sampling from our `ec_pool` and converting our samples to the corresponding rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ecs(ec_pool, sample_size):\n",
    "    # Safety check such that we do not take more samples than we can take\n",
    "    sample_size = min(sample_size, len(ec_pool))\n",
    "\n",
    "    # Sample experimental conditions\n",
    "    return RNG.choice(ec_pool, sample_size, replace=False)\n",
    "\n",
    "def compute_rankings(ecs, rank_matrix):\n",
    "    rm_ = rank_matrix.loc[:, ecs]\n",
    "    na, nv = rm_.shape\n",
    "    \n",
    "    # Generate rankings from the data\n",
    "    rankings = ru.SampleAM.from_rank_function_dataframe(rm_)\n",
    "    \n",
    "    return rankings, nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get compute the variance, variance lower bound and MMDs of these rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_and_lower_bound(rankings, n, kbar, kernel, kernelargs):\n",
    "    \n",
    "    variance = ku.var(rankings, use_rv=True, kernel=kernel, **kernelargs)\n",
    "    \n",
    "    var_lower_bound = gu.sample_mean_embedding_lowerbound(EPS, n, kbar=1, v=variance)\n",
    "    \n",
    "    return variance, var_lower_bound\n",
    "\n",
    "def calculate_mmds(rankings, nv, kernel, kernelargs):\n",
    "    mmds = {\n",
    "        n: mmd.subsample_mmd_distribution(\n",
    "            rankings, subsample_size=n, rep=100, use_rv=True, use_key=False,\n",
    "            seed=SEED, disjoint=DISJOINT, replace=REPLACE, kernel=kernel, **kernelargs\n",
    "        )\n",
    "        for n in range(2, nv // 2 + 1)\n",
    "    }\n",
    "    return mmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create dataframes consisting of Generalizability scores and Quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generalizability_dataframe(mmds, logepss):\n",
    "    ys = {n: [mmd.generalizability(mmde, np.exp(logeps)) for logeps in logepss] for n, mmde in mmds.items()}\n",
    "    dfy = pd.DataFrame(ys, index=logepss).reset_index().melt(id_vars='index', var_name='n', value_name='generalizability')\n",
    "    dfy.rename(columns={'index': 'log(eps)'}, inplace=True)\n",
    "    dfy['n'] = dfy['n'].astype(int)\n",
    "    return dfy\n",
    "\n",
    "def create_quantiles_dataframe(mmds):\n",
    "    qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
    "    dfq = pd.DataFrame(list(qs.items()), columns=['n', 'log(eps)'])\n",
    "    dfq['log(n)'] = np.log(dfq['n'])\n",
    "    return qs, dfq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the quantiles dataframe to calculate `nstar`. To this end, we fit linear regression models on subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_regression_with_cv(dfq):\n",
    "    # Extracting features and target from DataFrame\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    cv = KFold(n_splits=len(y))\n",
    "\n",
    "    residuals, linear_predictors = [], []\n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        lr = LinearRegression().fit(X[train_index], y[train_index])\n",
    "\n",
    "        predicted = lr.predict(X[test_index])\n",
    "        residuals.extend(y[test_index] - predicted)\n",
    "\n",
    "        linear_predictors.append(lr)\n",
    "\n",
    "    return linear_predictors, residuals\n",
    "\n",
    "def predict_nstar(logepss, linear_predictors, dfq):\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    ns_pred_cv = [np.exp(lr.predict(logepss.reshape(-1, 1)).reshape(-1)) for lr in linear_predictors]\n",
    "    \n",
    "    ns_pred = np.exp(LinearRegression().fit(X, y).predict(logepss.reshape(-1, 1)).reshape(-1))\n",
    "    \n",
    "    nstar_cv = [pred[np.argmax(logepss > np.log(EPS))] for pred in ns_pred_cv if not np.all(pred == 0)]\n",
    "    \n",
    "    nstar = ns_pred[np.argmax(logepss > np.log(EPS))]\n",
    "    \n",
    "    nstar_lower, nstar_upper = np.quantile(nstar_cv, [CI_LOWER, CI_UPPER])\n",
    "\n",
    "    return ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot the our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generalizability_and_quantiles(dfy, dfq, logepss, ns_pred, ns_pred_cv, mmds, qs, nstar, nstar_upper, nstar_lower, kernel_dir):\n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(2, 1, sharex=\"all\", figsize=(10, 8))\n",
    "\n",
    "    # Generalizability plot\n",
    "    ax = axes[0]\n",
    "    sns.lineplot(data=dfy, x=\"log(eps)\", y=\"generalizability\", hue=\"n\", ax=ax, palette=palette)\n",
    "    ax.hlines(ALPHA, ls=\"--\", xmin=np.min(logepss), xmax=np.max(logepss), color=\"black\")\n",
    "    for n in mmds.keys():\n",
    "        ax.vlines(qs[n], ymin=0, ymax=ALPHA, ls=\":\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # Quantiles plot\n",
    "    ax = axes[1]\n",
    "    ymax = max(ns_pred)\n",
    "    sns.lineplot(data=dfq, x=\"log(eps)\", y=\"n\", ax=ax, ls=\"\", marker=\"o\", hue=\"n\", legend=False)\n",
    "    for n in mmds.keys():\n",
    "        ax.vlines(qs[n], ymin=n, ymax=ymax, ls=\":\")\n",
    "    ax.vlines(np.log(EPS), ymin=0.1, ymax=ymax, color=\"black\", ls=\"--\")\n",
    "    sns.lineplot(x=logepss, y=ns_pred, color=\"green\", ls=\"-.\", ax=ax)\n",
    "    for it, ns_tmp in enumerate(ns_pred_cv):\n",
    "        if np.max(ns_tmp) > 1000:\n",
    "            continue\n",
    "        sns.lineplot(x=logepss, y=ns_tmp, color=\"green\", ls=\"-.\", alpha=0.5, ax=ax)\n",
    "\n",
    "    # N* Lines\n",
    "    ax.hlines(nstar, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\")\n",
    "    ax.hlines(nstar_upper, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "    ax.hlines(nstar_lower, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "    ax.set_yscale(\"log\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # Finalize and save\n",
    "    fig.suptitle(f\"Generalizability for N = {len(ecs):02d}\\n\"\n",
    "                 f\"n*(alpha={ALPHA}, eps={EPS}) = {np.ceil(nstar)}\\n\"\n",
    "                 f\"{LR_CONFIDENCE} confidence interval: [{np.ceil(nstar_lower)}, {np.ceil(nstar_upper)}]\")\n",
    "    plt.tight_layout()\n",
    "    if FORMAT == \"pdf\" or FORMAT == \"all\":\n",
    "        plt.savefig(kernel_dir / f\"N={len(ecs):02d}.pdf\")\n",
    "    if FORMAT == \"png\" or FORMAT == \"all\":\n",
    "        plt.savefig(kernel_dir / f\"N={len(ecs):02d}.png\")\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a0064736ab41438836f1a45c9fd059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configurations:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b5b402a97846d3b65d87149825299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Kernels:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1a1e78a88340aa9571f154262ecfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using mallows_kernel_nu_auto:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"133075563362304process_stream_events\"\n",
      "    while executing\n",
      "\"133075563362304process_stream_events\"\n",
      "    (\"after\" script)\n",
      "can't invoke \"event\" command: application has been destroyed\n",
      "    while executing\n",
      "\"event generate $w <<ThemeChanged>>\"\n",
      "    (procedure \"ttk::ThemeChanged\" line 6)\n",
      "    invoked from within\n",
      "\"ttk::ThemeChanged\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b5fd701cec4e62a35a8b1eef29990a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using jaccard_kernel_k_1:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e900f08292419da9447f1c113be5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using jaccard_kernel_k_2:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a877556cddd45da8783ba3090570b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using borda_kernel_idx_OHE:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accf218c86ba4803b6c9566a41682184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Kernels:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5abc348ce024150a3b516f8bd4fb31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using mallows_kernel_nu_auto:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cba0deea29843c7914a9a64ce88aae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using jaccard_kernel_k_1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6360e98ceb42e5a51840502ec92f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using jaccard_kernel_k_2:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ddc58f0a6340f8ae209e2973be54db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using borda_kernel_idx_OHE:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e94c193c7841e69bb72fa55044a70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Kernels:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48acc196a2cb437db5702d580c3023c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using mallows_kernel_nu_auto:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94bdeb8601942bca0291c51e7b480fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using jaccard_kernel_k_1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1e6269628644b38d8d5ce276f7e01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using jaccard_kernel_k_2:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02898fade5124d23a4d54c2616fbaa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Using borda_kernel_idx_OHE:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "\n",
    "for idf in tqdm(dataframes_list, desc='Configurations', leave=True):\n",
    "    factors_dict = {factor: idf[factor].unique()[0] for factor in EXPERIMENTAL_FACTORS}\n",
    "    # -- convert df to rank matrix\n",
    "    rank_matrix = ru.get_rankings_from_df(idf, factors=list(EXPERIMENTAL_FACTORS.keys()), \n",
    "                                            alternatives=ALTERNATIVES,\n",
    "                                            target=TARGET,\n",
    "                                            lower_is_better=False, impute_missing=True)\n",
    "    rank_matrix = rank_matrix.fillna(rank_matrix.max())\n",
    "\n",
    "    # -- get all kernels\n",
    "    kernels = init_kernels(rank_matrix)\n",
    "\n",
    "    # -- set up the ec pool\n",
    "    ec_variable = next((key for key, value in EXPERIMENTAL_FACTORS.items() if value is None), None)\n",
    "    ec_pool = idf[ec_variable].unique()\n",
    "    ecs = np.array([])\n",
    "    total_iterations = (len(ec_pool) + 9) // SAMPLE_SIZE\n",
    "\n",
    "    for kernelname, (kernel, kernelargs) in tqdm(kernels.items(), desc='Kernels', leave=False):\n",
    "        kernel_dir = create_experiment_directory(kernelname, factors_dict)\n",
    "        out = []\n",
    "        for i in tqdm(range(total_iterations), desc=f'Using {kernelname}', leave=False):\n",
    "            # -- Sample new rankings from ec pool\n",
    "            ecs = sample_ecs(ec_pool, (i+1)*SAMPLE_SIZE)\n",
    "            rankings, nv = compute_rankings(ecs, rank_matrix)\n",
    "\n",
    "            # -- Compute the lower bound\n",
    "            variance, var_lower_bound = compute_variance_and_lower_bound(rankings, n=len(ecs), kbar=1, kernel=kernel, kernelargs=kernelargs)\n",
    "\n",
    "            # -- Compute mmds\n",
    "            mmds = calculate_mmds(rankings, nv, kernel=kernel, kernelargs=kernelargs)\n",
    "\n",
    "            # -- Compute generalizability and quantiles\n",
    "\n",
    "            # Prepare log(eps) scale\n",
    "            logepss = np.linspace(np.log(EPS) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
    "\n",
    "            # Dataframe for generalizability\n",
    "            dfy = create_generalizability_dataframe(mmds, logepss)\n",
    "\n",
    "            # Dataframe for quantiles\n",
    "            qs, dfq = create_quantiles_dataframe(mmds)\n",
    "\n",
    "            # Linear Regression with Cross-Validation\n",
    "            linear_predictors, residuals = perform_linear_regression_with_cv(dfq)\n",
    "            \n",
    "            # Predictions\n",
    "            ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper = predict_nstar(logepss, linear_predictors, dfq)\n",
    "\n",
    "            # -- Plotting\n",
    "            plot_generalizability_and_quantiles(dfy, dfq, logepss, ns_pred, ns_pred_cv, mmds, qs, nstar, nstar_upper, nstar_lower, kernel_dir)\n",
    "\n",
    "            out.append({\n",
    "                \"kernel\": kernelname,\n",
    "                \"alpha\": ALPHA,\n",
    "                \"eps\": EPS,\n",
    "                \"disjoint\": DISJOINT,\n",
    "                \"replace\": REPLACE,\n",
    "                \"N\": len(ecs),\n",
    "                \"nstar\": nstar,\n",
    "                \"nstar_lower\": nstar_lower,\n",
    "                \"nstar_upper\": nstar_upper,\n",
    "                \"variance\": variance,\n",
    "                \"var_lower_bound\": var_lower_bound,\n",
    "            })\n",
    "        if FORMAT == \"gif\" or FORMAT == \"all\":\n",
    "            images = [iio.imread(image) for image in glob.glob(str(kernel_dir / \"*.png\"))]\n",
    "            iio.mimwrite(kernel_dir / f\"nstar.gif\", images, duration=750,\n",
    "                            loop=0)\n",
    "        # -- Store nstar predictions\n",
    "        out = pd.DataFrame(out)\n",
    "        out.to_parquet(kernel_dir / \"nstar.parquet\")\n",
    "plt.ion()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
