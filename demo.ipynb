{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of core features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports and some plot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio.v2 as iio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from numpy.random import Generator\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import src.lower_bounds as gu\n",
    "import src.kernels as ku\n",
    "import src.probability_distributions as prob\n",
    "import src.rankings_utils as ru\n",
    "import src.mmd as mmd\n",
    "\n",
    "mpl.use(\"TkAgg\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"notebook\")\n",
    "palette = \"flare_r\"\n",
    "sns.set_palette(\"flare_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the hyperparameters from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/benchmark_encoders\")\n",
    "FIGURES_DIR = Path(\"Figures\")\n",
    "\n",
    "# gif for png & gif, pdf for pdf\n",
    "FORMAT = \"gif\"\n",
    "\n",
    "\n",
    "EPS = 0.2  # max MMD is sqrt2 sqrt(kbar), i.e., sqrt2 in most applications\n",
    "ALPHA = 0.95  # in 95% of cases, we'll get similar experimental results\n",
    "\n",
    "# Decide on the CI for N*\n",
    "LR_CONFIDENCE = 0.9\n",
    "CI_LOWER = (1 - LR_CONFIDENCE)/2\n",
    "CI_UPPER = LR_CONFIDENCE + CI_LOWER # confidence interval for linear prediction\n",
    "\n",
    "seed = 1444\n",
    "\n",
    "DATA_SET = Path(DATA_DIR / \"results.parquet\")\n",
    "\n",
    "# Set query params\n",
    "experimental_conditions ={\n",
    "    \"dataset\": None,\n",
    "    \"model\": \"LR\",\n",
    "    \"tuning\": \"no\",\n",
    "    \"scoring\": \"ACC\"\n",
    "}\n",
    "\n",
    "target = \"cv_score\"\n",
    "alternatives = \"encoder\"\n",
    "\n",
    "disjoint = True\n",
    "replace = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the data set into a pandas data frame and we check that only one value in the experimental conditions is set to None, indicating that it is variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET.suffix == '.parquet':\n",
    "    df = pd.read_parquet(DATA_SET)\n",
    "elif DATA_SET.suffix == '.csv':\n",
    "    df = pd.read_csv(DATA_SET)\n",
    "else:\n",
    "    raise Exception(\"Please use a Parquet or CSV file as the format of your data\")\n",
    "\n",
    "assert sum(value is None for value in experimental_conditions.values()) == 1, \"Exactly one element must be None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a query from the experimental conditions dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \" and \".join(f\"{key} == '{value}'\" if isinstance(value, str) else f\"{key} == {value}\"\n",
    "                            for key, value in experimental_conditions.items() if value is not None)\n",
    "\n",
    "# Check if query params exist in the df\n",
    "columns_to_check = set(experimental_conditions.keys()).union({target, alternatives})\n",
    "missing_columns = columns_to_check - set(df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following columns are missing from the dataframe: {missing_columns}\")\n",
    "\n",
    "# Build query\n",
    "df = df.query(query_string).reset_index(drop=True)\n",
    "rf = ru.get_rankings_from_df(df, factors=list(experimental_conditions.keys()), alternatives=alternatives,\n",
    "                             target=target,\n",
    "                             lower_is_better=False, impute_missing=True)\n",
    "rf = rf.fillna(rf.max())\n",
    "\n",
    "#TODO make more general\n",
    "# Create a directory for this run\n",
    "EXP0_DIR = FIGURES_DIR / \"Proof_of_concept_5\" / \"encoders_{model}_{tuning}_{scoring}\".format(**experimental_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the kernels that we want to use for MMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = {\n",
    "    \"mallows_auto\": (ku.mallows_kernel, {\"nu\": \"auto\"}),\n",
    "    \"jaccard_1\": (ku.jaccard_kernel, {\"k\": 1}),\n",
    "    \"borda_OHE\": (ku.borda_kernel, {\"idx\": rf.index.get_loc(\"OHE\")}),\n",
    "    \"borda_DE\": (ku.borda_kernel, {\"idx\": rf.index.get_loc(\"DE\")})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP1_DIR = EXP0_DIR / f\"{kernelname}\"\n",
    "\n",
    "# ---- Create directories\n",
    "EXP2_DIR = EXP1_DIR / f\"nstar_N_ALPHA={ALPHA}_eps={EPS}_ci={LR_CONFIDENCE}_disjoint={disjoint}_replace={replace}\"\n",
    "for ED in [EXP0_DIR, EXP1_DIR, EXP2_DIR]:\n",
    "    ED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Computation\n",
    "ec_variable = next((key for key, value in experimental_conditions.items() if value is None), None)\n",
    "print(ec_variable)\n",
    "ec_pool = df[ec_variable].unique()  # we remove experimental conditions from it to simulate running new experiments\n",
    "ecs = np.array([])  # ecs on which we have already run experiments\n",
    "out = []\n",
    "plt.ioff()\n",
    "while len(ec_pool) > 0:\n",
    "    # -- Sample a new dataset\n",
    "    # initialization with the minimum meaningful n, 4 if disjoint is True\n",
    "    if len(ecs) == 0:\n",
    "        ecs = np.random.default_rng(seed).choice(ec_pool, 10, replace=False)\n",
    "    else:\n",
    "        ecs = np.append(ecs,\n",
    "                                np.random.default_rng(seed).choice(ec_pool, 10, replace=False))\n",
    "    ec_pool = np.setdiff1d(ec_pool, ecs)  # remove the sampled ecs rom the pool\n",
    "\n",
    "    rf_ = rf.loc[:, ecs]\n",
    "    na, nv = rf_.shape\n",
    "    rankings = ru.SampleAM.from_rank_function_dataframe(rf_)\n",
    "\n",
    "    # -- Compute the lower bound\n",
    "    variance = ku.var(rankings, use_rf=True, kernel=kernel, **kernelargs)\n",
    "    var_lower_bound = gu.sample_mean_embedding_lowerbound(EPS, len(ecs), kbar=1,\n",
    "                                                            v=variance)\n",
    "\n",
    "    # -- Compute mmds\n",
    "    mmds = {\n",
    "        n: mmd.subsample_mmd_distribution(rankings, subsample_size=n, rep=100,\n",
    "                                            use_rf=True, use_key=False, seed=seed,\n",
    "                                            disjoint=disjoint, replace=replace,\n",
    "                                            kernel=kernel, **kernelargs)\n",
    "        for n in range(2, nv // 2 + 1)\n",
    "    }\n",
    "\n",
    "    # -- Compute generalizability and quantiles\n",
    "    # Prepare log(eps) scale\n",
    "    logepss = np.linspace(np.log(EPS) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
    "\n",
    "    # Compute generalizability and quantiles\n",
    "    ys = {n: [mmd.generalizability(mmde, np.exp(logeps)) for logeps in logepss] for n, mmde in mmds.items()}\n",
    "    qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
    "\n",
    "    # Dataframe for generalizability\n",
    "    dfy = pd.DataFrame(ys, index=logepss).reset_index().melt(id_vars='index', var_name='n', value_name='generalizability')\n",
    "    dfy.rename(columns={'index': 'log(eps)'}, inplace=True)\n",
    "    dfy['n'] = dfy['n'].astype(int)\n",
    "\n",
    "    # Dataframe for quantiles\n",
    "    dfq = pd.DataFrame(list(qs.items()), columns=['n', 'log(eps)'])\n",
    "    dfq['log(n)'] = np.log(dfq['n'])\n",
    "\n",
    "    # Linear Regression with Cross-Validation\n",
    "    X, y = dfq[['log(eps)']].values, dfq[['log(n)']].values\n",
    "    cv = KFold(n_splits=len(y))\n",
    "\n",
    "    residuals, linear_predictors = [], []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        lr = LinearRegression().fit(X[train_index], y[train_index])\n",
    "        residuals.extend(y[test_index] - lr.predict(X[test_index]))\n",
    "        linear_predictors.append(lr)\n",
    "\n",
    "    # Predictions\n",
    "    ns_pred_cv = [np.exp(lr.predict(logepss.reshape(-1, 1)).reshape(-1)) for lr in linear_predictors]\n",
    "    ns_pred = np.exp(LinearRegression().fit(X, y).predict(logepss.reshape(-1, 1)).reshape(-1))\n",
    "\n",
    "    nstar_cv = [pred[np.argmax(logepss > np.log(EPS))] for pred in ns_pred_cv if not np.all(pred == 0)]\n",
    "    nstar = ns_pred[np.argmax(logepss > np.log(EPS))]\n",
    "\n",
    "    nstar_lower, nstar_upper = np.quantile(nstar_cv, [0.05, 0.95])\n",
    "\n",
    "    print(\"N*: \", nstar)\n",
    "    print(f\"N* CI from {CI_LOWER} to {CI_UPPER}: [{nstar_lower}, {nstar_upper}]\")\n",
    "\n",
    "    # -- Plot\n",
    "    fig, axes = plt.subplots(2, 1, sharex=\"all\", figsize=(10, 8))\n",
    "\n",
    "    # - Generalizability\n",
    "    ax = axes[0]\n",
    "    sns.lineplot(dfy, x=\"log(eps)\", y=\"generalizability\", hue=\"n\", ax=ax, palette=palette)\n",
    "    ax.hlines(ALPHA, ls=\"--\", xmin=np.min(logepss), xmax=np.max(logepss), color=\"black\")\n",
    "    for n in mmds.keys():\n",
    "        ax.vlines(qs[n], ymin=0, ymax=ALPHA, ls=\":\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # - Quantiles\n",
    "    ax = axes[1]\n",
    "    ymax = max(ns_pred)\n",
    "    sns.lineplot(dfq, x=\"log(eps)\", y=\"n\", ax=ax, ls=\"\", marker=\"o\", hue=\"n\", legend=False)\n",
    "    for n in mmds.keys():\n",
    "        ax.vlines(qs[n], ymin=n, ymax=ymax, ls=\":\")\n",
    "    ax.vlines(np.log(EPS), ymin=0.1, ymax=ymax, color=\"black\", ls=\"--\")\n",
    "\n",
    "    # - Linear regression\n",
    "    sns.lineplot(x=logepss, y=ns_pred, color=\"green\", ls=\"-.\", ax=ax)\n",
    "    for it, ns_tmp in enumerate(ns_pred_cv):\n",
    "        if np.max(ns_tmp) > 1000:  # TODO: hard-coded threshold for broken confidence intervals, make it more reliable\n",
    "            continue\n",
    "        sns.lineplot(x=logepss, y=ns_tmp, color=\"green\", ls=\"-.\", alpha=0.5, ax=ax)\n",
    "\n",
    "    # - N*\n",
    "    ax.hlines(nstar, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\")\n",
    "    ax.hlines(nstar_upper, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "    ax.hlines(nstar_lower, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "\n",
    "    ax.set_yscale(\"log\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # - Finalize\n",
    "    fig.suptitle(f\"Generalizability for N = {len(ecs):02d}\\n\"\n",
    "                    f\"n*(alpha={ALPHA}, eps={EPS}) = {np.ceil(nstar)}\\n\"\n",
    "                    f\"{LR_CONFIDENCE} confidence interval: [{np.ceil(nstar_lower)}, {np.ceil(nstar_upper)}]\")\n",
    "    plt.tight_layout()\n",
    "    if FORMAT == \"pdf\":\n",
    "        plt.savefig(EXP2_DIR / f\"N={len(ecs):02d}.pdf\")\n",
    "    else:\n",
    "        plt.savefig(EXP2_DIR / f\"N={len(ecs):02d}.png\")\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    out.append({\n",
    "        \"kernel\": kernelname,\n",
    "        \"alpha\": ALPHA,\n",
    "        \"eps\": EPS,\n",
    "        \"disjoint\": disjoint,\n",
    "        \"replace\": replace,\n",
    "        \"N\": len(ecs),\n",
    "        \"nstar\": nstar,\n",
    "        \"nstar_lower\": nstar_lower,\n",
    "        \"nstar_upper\": nstar_upper,\n",
    "        \"variance\": variance,\n",
    "        \"var_lower_bound\": var_lower_bound,\n",
    "    })\n",
    "plt.ion()\n",
    "if FORMAT == \"gif\":\n",
    "    images = [iio.imread(image) for image in glob.glob(str(EXP2_DIR / \"*.png\"))]\n",
    "    iio.mimwrite(EXP2_DIR / f\"nstar.gif\", images, duration=750,\n",
    "                    loop=0)\n",
    "\n",
    "# -- Store nstar predictions\n",
    "out = pd.DataFrame(out)\n",
    "out.to_parquet(EXP2_DIR / \"nstar.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
