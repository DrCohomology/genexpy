{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of core features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports and some plot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio.v2 as iio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from numpy.random import Generator\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import src.lower_bounds as gu\n",
    "import src.kernels as ku\n",
    "import src.probability_distributions as prob\n",
    "import src.rankings_utils as ru\n",
    "import src.mmd as mmd\n",
    "\n",
    "mpl.use(\"TkAgg\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"notebook\")\n",
    "palette = \"flare_r\"\n",
    "sns.set_palette(\"flare_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the hyperparameters from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "DATA_DIR = Path(config['paths']['data_dir'])\n",
    "FIGURES_DIR = Path(config['paths']['figures_dir'])\n",
    "\n",
    "FORMAT = config['format']['output']\n",
    "\n",
    "SEED = config['statistics']['seed']\n",
    "EPS = config['statistics']['epsilon']\n",
    "ALPHA = config['statistics']['alpha']\n",
    "LR_CONFIDENCE = config['statistics']['lr_confidence']\n",
    "CI_LOWER = (1 - LR_CONFIDENCE) / 2\n",
    "CI_UPPER = LR_CONFIDENCE + CI_LOWER\n",
    "\n",
    "DATA_SET = Path(config['data']['dataset_path'])\n",
    "EXPERIMENTAL_CONDITIONS = config['data']['experimental_conditions']\n",
    "TARGET = config['data']['target']\n",
    "ALTERNATIVES = config['data']['alternatives']\n",
    "\n",
    "disjoint = config['experiment']['disjoint']\n",
    "replace = config['experiment']['replace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the data set into a pandas data frame and we check that only one value in the experimental conditions is set to `None`, indicating that it is variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET.suffix == '.parquet':\n",
    "    df = pd.read_parquet(DATA_SET)\n",
    "elif DATA_SET.suffix == '.csv':\n",
    "    df = pd.read_csv(DATA_SET)\n",
    "else:\n",
    "    raise Exception(\"Please use a Parquet or CSV file as the format of your data\")\n",
    "\n",
    "assert sum(value is None for value in EXPERIMENTAL_CONDITIONS.values()) == 1, \"Exactly one element must be None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a query from the experimental conditions dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \" and \".join(f\"{key} == '{value}'\" if isinstance(value, str) else f\"{key} == {value}\"\n",
    "                            for key, value in EXPERIMENTAL_CONDITIONS.items() if value is not None)\n",
    "\n",
    "# Check if query params exist in the df\n",
    "columns_to_check = set(EXPERIMENTAL_CONDITIONS.keys()).union({TARGET, ALTERNATIVES})\n",
    "missing_columns = columns_to_check - set(df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following columns are missing from the dataframe: {missing_columns}\")\n",
    "\n",
    "# Build query\n",
    "df = df.query(query_string).reset_index(drop=True)\n",
    "rv = ru.get_rankings_from_df(df, factors=list(EXPERIMENTAL_CONDITIONS.keys()), alternatives=ALTERNATIVES,\n",
    "                             target=TARGET,\n",
    "                             lower_is_better=False, impute_missing=True)\n",
    "rv = rv.fillna(rv.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some kernels that cana be used for MMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = {\n",
    "    \"mallows_auto\": (ku.mallows_kernel, {\"nu\": \"auto\"}),\n",
    "    \"jaccard_1\": (ku.jaccard_kernel, {\"k\": 1}),\n",
    "    #\"borda_OHE\": (ku.borda_kernel, {\"idx\": rv.index.get_loc(\"OHE\")}),\n",
    "    #\"borda_DE\": (ku.borda_kernel, {\"idx\": rv.index.get_loc(\"DE\")})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will use the mallows kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelname = \"mallows_auto\"\n",
    "kernel, kernelargs = kernels[kernelname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some directories for the different experiments we run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Create directories\n",
    "exp0_dir_name_parts = [f\"name={ALTERNATIVES}\"] + [f\"{key}={value}\" for key, value in EXPERIMENTAL_CONDITIONS.items() if value is not None]\n",
    "EXP0_DIR = FIGURES_DIR / \"_\".join(exp0_dir_name_parts)\n",
    "EXP1_DIR = EXP0_DIR / f\"{kernelname}\"\n",
    "EXP2_DIR = EXP1_DIR / f\"nstar_N_ALPHA={ALPHA}_eps={EPS}_ci={LR_CONFIDENCE}_disjoint={disjoint}_replace={replace}\"\n",
    "for ED in [EXP0_DIR, EXP1_DIR, EXP2_DIR]:\n",
    "    ED.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the experimental conditions pool to simulate running new experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x79ce4e2b86e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Computation\n",
    "ec_variable = next((key for key, value in EXPERIMENTAL_CONDITIONS.items() if value is None), None)\n",
    "ec_pool = df[ec_variable].unique()  # we remove experimental conditions from it to simulate running new experiments\n",
    "ecs = np.array([])  # ecs on which we have already run experiments\n",
    "out = []\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(ec_pool) > 0:\n",
    "    # -- Sample a new dataset\n",
    "    # initialization with the minimum meaningful n, 4 if disjoint is True\n",
    "    if len(ecs) == 0:\n",
    "        ecs = np.random.default_rng(SEED).choice(ec_pool, 10, replace=False)\n",
    "    else:\n",
    "        ecs = np.append(ecs, np.random.default_rng(SEED).choice(ec_pool, 10, replace=False))\n",
    "    ec_pool = np.setdiff1d(ec_pool, ecs)  # remove the sampled ecs rom the pool\n",
    "\n",
    "    rv_ = rv.loc[:, ecs]\n",
    "    na, nv = rv_.shape\n",
    "    rankings = ru.SampleAM.from_rank_function_dataframe(rv_)\n",
    "\n",
    "    # -- Compute the lower bound\n",
    "    variance = ku.var(rankings, use_rv=True, kernel=kernel, **kernelargs)\n",
    "    var_lower_bound = gu.sample_mean_embedding_lowerbound(EPS, len(ecs), kbar=1,\n",
    "                                                            v=variance)\n",
    "\n",
    "    # -- Compute mmds\n",
    "    mmds = {\n",
    "        n: mmd.subsample_mmd_distribution(rankings, subsample_size=n, rep=100,\n",
    "                                            use_rv=True, use_key=False, seed=SEED,\n",
    "                                            disjoint=disjoint, replace=replace,\n",
    "                                            kernel=kernel, **kernelargs)\n",
    "        for n in range(2, nv // 2 + 1)\n",
    "    }\n",
    "\n",
    "    # -- Compute generalizability and quantiles\n",
    "\n",
    "    # Prepare log(eps) scale\n",
    "    logepss = np.linspace(np.log(EPS) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
    "\n",
    "    # Compute generalizability and quantiles\n",
    "    ys = {n: [mmd.generalizability(mmde, np.exp(logeps)) for logeps in logepss] for n, mmde in mmds.items()}\n",
    "    qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
    "\n",
    "    # Dataframe for generalizability\n",
    "    dfy = pd.DataFrame(ys, index=logepss).reset_index().melt(id_vars='index', var_name='n', value_name='generalizability')\n",
    "    dfy.rename(columns={'index': 'log(eps)'}, inplace=True)\n",
    "    dfy['n'] = dfy['n'].astype(int)\n",
    "\n",
    "    # Dataframe for quantiles\n",
    "    dfq = pd.DataFrame(list(qs.items()), columns=['n', 'log(eps)'])\n",
    "    dfq['log(n)'] = np.log(dfq['n'])\n",
    "\n",
    "    # Linear Regression with Cross-Validation\n",
    "    X, y = dfq[['log(eps)']].values, dfq[['log(n)']].values\n",
    "    cv = KFold(n_splits=len(y))\n",
    "\n",
    "    residuals, linear_predictors = [], []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        lr = LinearRegression().fit(X[train_index], y[train_index])\n",
    "        residuals.extend(y[test_index] - lr.predict(X[test_index]))\n",
    "        linear_predictors.append(lr)\n",
    "\n",
    "    # Predictions\n",
    "    ns_pred_cv = [np.exp(lr.predict(logepss.reshape(-1, 1)).reshape(-1)) for lr in linear_predictors]\n",
    "    ns_pred = np.exp(LinearRegression().fit(X, y).predict(logepss.reshape(-1, 1)).reshape(-1))\n",
    "\n",
    "    nstar_cv = [pred[np.argmax(logepss > np.log(EPS))] for pred in ns_pred_cv if not np.all(pred == 0)]\n",
    "    nstar = ns_pred[np.argmax(logepss > np.log(EPS))]\n",
    "\n",
    "    nstar_lower, nstar_upper = np.quantile(nstar_cv, [0.05, 0.95])\n",
    "\n",
    "    print(\"N*: \", nstar)\n",
    "    print(f\"N* CI from {CI_LOWER} to {CI_UPPER}: [{nstar_lower}, {nstar_upper}]\")\n",
    "\n",
    "    # -- Plot\n",
    "    fig, axes = plt.subplots(2, 1, sharex=\"all\", figsize=(10, 8))\n",
    "\n",
    "    # - Generalizability\n",
    "    ax = axes[0]\n",
    "    sns.lineplot(dfy, x=\"log(eps)\", y=\"generalizability\", hue=\"n\", ax=ax, palette=palette)\n",
    "    ax.hlines(ALPHA, ls=\"--\", xmin=np.min(logepss), xmax=np.max(logepss), color=\"black\")\n",
    "    for n in mmds.keys():\n",
    "        ax.vlines(qs[n], ymin=0, ymax=ALPHA, ls=\":\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # - Quantiles\n",
    "    ax = axes[1]\n",
    "    ymax = max(ns_pred)\n",
    "    sns.lineplot(dfq, x=\"log(eps)\", y=\"n\", ax=ax, ls=\"\", marker=\"o\", hue=\"n\", legend=False)\n",
    "    for n in mmds.keys():\n",
    "        ax.vlines(qs[n], ymin=n, ymax=ymax, ls=\":\")\n",
    "    ax.vlines(np.log(EPS), ymin=0.1, ymax=ymax, color=\"black\", ls=\"--\")\n",
    "\n",
    "    # - Linear regression\n",
    "    sns.lineplot(x=logepss, y=ns_pred, color=\"green\", ls=\"-.\", ax=ax)\n",
    "    for it, ns_tmp in enumerate(ns_pred_cv):\n",
    "        if np.max(ns_tmp) > 1000:  # TODO: hard-coded threshold for broken confidence intervals, make it more reliable\n",
    "            continue\n",
    "        sns.lineplot(x=logepss, y=ns_tmp, color=\"green\", ls=\"-.\", alpha=0.5, ax=ax)\n",
    "\n",
    "    # - N*\n",
    "    ax.hlines(nstar, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\")\n",
    "    ax.hlines(nstar_upper, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "    ax.hlines(nstar_lower, xmin=np.min(logepss), xmax=np.log(EPS), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "\n",
    "    ax.set_yscale(\"log\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # - Finalize\n",
    "    fig.suptitle(f\"Generalizability for N = {len(ecs):02d}\\n\"\n",
    "                    f\"n*(alpha={ALPHA}, eps={EPS}) = {np.ceil(nstar)}\\n\"\n",
    "                    f\"{LR_CONFIDENCE} confidence interval: [{np.ceil(nstar_lower)}, {np.ceil(nstar_upper)}]\")\n",
    "    plt.tight_layout()\n",
    "    if FORMAT == \"pdf\":\n",
    "        plt.savefig(EXP2_DIR / f\"N={len(ecs):02d}.pdf\")\n",
    "    else:\n",
    "        plt.savefig(EXP2_DIR / f\"N={len(ecs):02d}.png\")\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    out.append({\n",
    "        \"kernel\": kernelname,\n",
    "        \"alpha\": ALPHA,\n",
    "        \"eps\": EPS,\n",
    "        \"disjoint\": disjoint,\n",
    "        \"replace\": replace,\n",
    "        \"N\": len(ecs),\n",
    "        \"nstar\": nstar,\n",
    "        \"nstar_lower\": nstar_lower,\n",
    "        \"nstar_upper\": nstar_upper,\n",
    "        \"variance\": variance,\n",
    "        \"var_lower_bound\": var_lower_bound,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "if FORMAT == \"gif\":\n",
    "    images = [iio.imread(image) for image in glob.glob(str(EXP2_DIR / \"*.png\"))]\n",
    "    iio.mimwrite(EXP2_DIR / f\"nstar.gif\", images, duration=750,\n",
    "                    loop=0)\n",
    "\n",
    "# -- Store nstar predictions\n",
    "out = pd.DataFrame(out)\n",
    "out.to_parquet(EXP2_DIR / \"nstar.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
