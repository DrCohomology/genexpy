{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many datasets are enough?\n",
    "\n",
    "As explained in the paper, given a desired generalizability $\\alpha^*$ and a similarity threshold $\\delta^*$, we can estimate the number of datasets required to obtain generalizable results. \n",
    "The procedure goes as follows: \n",
    "1. Load the experimental results. \n",
    "2. Query the results for a combination of design and held-constant factors.\n",
    "3. Sample without repetition $N$ levels of the allowed-to-vary factor and query the results accordingly. \n",
    "4. Rank the alternatives according to the target column. \n",
    "5. Estimate the $\\alpha^*$-quantile of MMD for all $n \\leq N$, call it $\\varepsilon^{\\alpha^*}_n$.\n",
    "6. Fit a linear model $\\log(n) = \\beta_1 \\log(\\varepsilon^{\\alpha^*}_n) + \\beta_0$.\n",
    "7. Estimate $n^*$ with the linear model.\n",
    "\n",
    "Most of these operations are performed in the main loop. Before that, however, we need to import the modules we'll need, load the configuration file, and define some utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T14:15:02.129235Z",
     "start_time": "2024-05-28T14:15:02.122315Z"
    }
   },
   "source": [
    "import glob\n",
    "import imageio.v2 as iio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from numpy.random import Generator\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from genexpy import lower_bounds as gu\n",
    "from genexpy import kernels as ku\n",
    "from genexpy import probability_distributions as prob\n",
    "from genexpy import rankings_utils as ru\n",
    "from genexpy import mmd as mmd"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the hyperparameters from the config."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T14:15:32.626705Z",
     "start_time": "2024-05-28T14:15:32.607904Z"
    }
   },
   "source": [
    "with open(\"config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "OUTPUT_DIR = Path(config['paths']['output_dir'])\n",
    "FIGURES_DIR = Path(config['paths']['figures_dir'])\n",
    "\n",
    "FORMAT = config['format']['figure_output']\n",
    "\n",
    "SEED = config['parameters']['seed']\n",
    "RNG = np.random.default_rng(SEED)\n",
    "ALPHA = config['parameters']['alpha']\n",
    "LR_CONFIDENCE = config['parameters']['lr_confidence']\n",
    "CI_LOWER = (1 - LR_CONFIDENCE) / 2\n",
    "CI_UPPER = LR_CONFIDENCE + CI_LOWER\n",
    "\n",
    "DATASET = Path(config['data']['dataset_path'])\n",
    "EXPERIMENTAL_FACTORS = config['data']['experimental_factors']\n",
    "TARGET = config['data']['target']\n",
    "ALTERNATIVES = config['data']['alternatives']\n",
    "\n",
    "SAMPLE_SIZE = config['sampling']['sample_size']\n",
    "DISJOINT = config['sampling']['disjoint']\n",
    "REPLACE = config['sampling']['replace']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We now load the dataset of results and check that *exactly one* of the experimental factors is allowed-to-vary. "
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET.suffix == '.parquet':\n",
    "    df = pd.read_parquet(DATASET).drop(columns=[\"time\"])\n",
    "elif DATASET.suffix == '.csv':\n",
    "    df = pd.read_csv(DATASET).drop(columns=[\"time\"])\n",
    "else:\n",
    "    raise Exception(\"Please use a Parquet or CSV file as the format of your data\")\n",
    "\n",
    "# Check whether exactly one of the experimental factors is None \n",
    "assert sum(value is None for value in EXPERIMENTAL_FACTORS.values()) == 1, \"Exactly one experimental factor must be set to null in config.yaml.\"\n",
    "\n",
    "# Check whether the factors listed in the config are the columns of df\n",
    "columns_to_check = set(EXPERIMENTAL_FACTORS.keys()).union({TARGET, ALTERNATIVES})\n",
    "if not_in_df := columns_to_check - set(df.columns):\n",
    "    raise ValueError(f\"The following columns are missing from the dataframe: {not_in_df}\")\n",
    "if not_in_config:= set(df.columns) - columns_to_check:\n",
    "    raise ValueError(f\"The following columns in the dataframe are not required: {not_in_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a query from the experimental factors and convert the `df`. Note that here, we assume that we are using the _all key to designate that we want to use all possible values for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "# query df for the fixed factor levels in config.yaml\n",
    "try:\n",
    "    query_string = \" and \".join(f\"{factor} == '{lvl}'\" if isinstance(lvl, str) else f\"{factor} == {lvl}\"\n",
    "                                for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                                if lvl not in [None, \"_all\"])\n",
    "    df = df.query(query_string)\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "# for the not allowed-to-vary factors, get their combinations\n",
    "try:\n",
    "    groups = df.groupby([factor for factor, lvl in EXPERIMENTAL_FACTORS.items() if lvl == \"_all\"]).groups\n",
    "except ValueError:\n",
    "    groups = {\"None\": df.index}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the kernels that we defined in the config. We will later iterate over them, as to perform our analysis over all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kernels(rank_matrix):\n",
    "    kernels = {}\n",
    "    for kernel_config in config['kernels']:\n",
    "        kernel_func = getattr(ku, kernel_config['kernel'], None)\n",
    "        \n",
    "        if kernel_func:\n",
    "            delta = kernel_config['delta']  # to get epsilon\n",
    "            match kernel_config['kernel']:\n",
    "                case \"mallows_kernel\":\n",
    "                    eps = np.sqrt(2 * (1 - np.exp(-delta)))  # assumes nu = 1/binom(n, 2)\n",
    "                case \"jaccard_kernel\":\n",
    "                    eps = np.sqrt(2 * (1 - (1-delta)))\n",
    "                case \"borda_kernel\":\n",
    "                    eps = np.sqrt(2 * (1 - np.exp(-delta)))   # assumes nu = 1/n\n",
    "                case _ :\n",
    "                    raise ValueError(f\"The kernel {kernel_config['kernel']} must be either the Jaccard, Mallows, or Borda kernel.\")\n",
    "\n",
    "            for param_key, param_values in kernel_config['params'].items():\n",
    "                if isinstance(param_values, list):\n",
    "                    for value in param_values:\n",
    "                        params = {param_key: value}\n",
    "                        if param_key == 'idx':\n",
    "                            params[param_key] = rank_matrix.index.get_loc(value)\n",
    "\n",
    "                        kernel_name = f\"{kernel_config['kernel']}_{param_key}_{value}\"\n",
    "                        kernels[kernel_name] = (kernel_func, params, eps, delta)\n",
    "                else:\n",
    "                    params = {param_key: param_values}\n",
    "                    if param_key == 'idx':\n",
    "                        params[param_key] = rank_matrix.index.get_loc(param_values)\n",
    "\n",
    "                    kernel_name = f\"{kernel_config['kernel']}_{param_key}_{param_values}\"\n",
    "                    kernels[kernel_name] = (kernel_func, params, eps, delta)\n",
    "        else:\n",
    "            print(f\"Kernel function '{kernel_config['kernel']}' not found in module 'kernels'.\")\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some directories for the different experiments we run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_directory(kernel_name, factors, delta):\n",
    "    exp0_dir = OUTPUT_DIR / \"_\".join([f\"{key}={value}\" for key, value in factors.items() if value is not None])\n",
    "    exp1_dir = exp0_dir / f\"{kernel_name}\"\n",
    "    exp21_dir = exp1_dir / f\"nstar_N_ALPHA={ALPHA}_delta={delta}_ci={LR_CONFIDENCE}\"\n",
    "    exp21_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exp22_dir = exp1_dir / \"computed_generalizability\"\n",
    "    exp22_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exp23_dir = exp1_dir / \"computed_quantiles\"\n",
    "    exp23_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return exp21_dir, exp22_dir, exp23_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a few methods that will be run in a loop until our `ec_pool` is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by sampling from our `ec_pool` and converting our samples to the corresponding rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ecs(ec_pool, sample_size):\n",
    "    assert sample_size <= len(ec_pool), f\"Sample size {sample_size} is larger than |ec_pool| = {len(ec_pool)}\"\n",
    "\n",
    "    # Sample experimental conditions\n",
    "    return RNG.choice(ec_pool, sample_size, replace=False)\n",
    "\n",
    "def compute_rankings(ecs, rank_matrix):\n",
    "    rm_ = rank_matrix.loc[:, ecs]\n",
    "    na, nv = rm_.shape\n",
    "    \n",
    "    # Generate rankings from the data\n",
    "    rankings = ru.SampleAM.from_rank_function_dataframe(rm_)\n",
    "    \n",
    "    return rankings, nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get compute the variance, variance lower bound and MMDs of these rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_and_lower_bound(rankings, n, kbar, eps, kernel, kernelargs):\n",
    "    variance = ku.var(rankings, use_rv=True, kernel=kernel, **kernelargs)\n",
    "    var_lower_bound = gu.sample_mean_embedding_lowerbound(eps, n, kbar=1, v=variance)\n",
    "    return variance, var_lower_bound\n",
    "\n",
    "def calculate_mmds(rankings, nv, kernel, kernelargs):\n",
    "    mmds = {\n",
    "        n: mmd.subsample_mmd_distribution(\n",
    "            rankings, subsample_size=n, rep=100, use_rv=True, use_key=False,\n",
    "            seed=SEED, disjoint=DISJOINT, replace=REPLACE, kernel=kernel, **kernelargs\n",
    "        )\n",
    "        for n in range(2, nv // 2 + 1)\n",
    "    }\n",
    "    return mmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create dataframes consisting of Generalizability scores and Quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generalizability_dataframe(mmds, logepss):\n",
    "    ys = {n: [mmd.generalizability(mmde, np.exp(logeps)) for logeps in logepss] for n, mmde in mmds.items()}\n",
    "    dfy = pd.DataFrame(ys, index=logepss).reset_index().melt(id_vars='index', var_name='n', value_name='generalizability')\n",
    "    dfy.rename(columns={'index': 'log(eps)'}, inplace=True)\n",
    "    dfy['n'] = dfy['n'].astype(int)\n",
    "    return dfy\n",
    "\n",
    "def create_quantiles_dataframe(mmds):\n",
    "    qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
    "    dfq = pd.DataFrame(list(qs.items()), columns=['n', 'log(eps)'])\n",
    "    dfq['log(n)'] = np.log(dfq['n'])\n",
    "    return dfq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the quantiles dataframe to calculate `nstar`. To this end, we fit linear regression models on subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_regression_with_cv(dfq):\n",
    "    # Extracting features and target from DataFrame\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    cv = KFold(n_splits=len(y))\n",
    "\n",
    "    residuals, linear_predictors = [], []\n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        lr = LinearRegression().fit(X[train_index], y[train_index])\n",
    "\n",
    "        predicted = lr.predict(X[test_index])\n",
    "        residuals.extend(y[test_index] - predicted)\n",
    "\n",
    "        linear_predictors.append(lr)\n",
    "\n",
    "    return linear_predictors, residuals\n",
    "\n",
    "def predict_nstar(logepss, linear_predictors, dfq, eps):\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    ns_pred_cv = [np.exp(lr.predict(logepss.reshape(-1, 1)).reshape(-1)) for lr in linear_predictors]\n",
    "\n",
    "    ns_pred = np.exp(LinearRegression().fit(X, y).predict(logepss.reshape(-1, 1)).reshape(-1))\n",
    "\n",
    "    nstar_cv = [pred[np.argmax(logepss > np.log(eps))] for pred in ns_pred_cv if not np.all(pred == 0)]\n",
    "\n",
    "    nstar = ns_pred[np.argmax(logepss > np.log(eps))]\n",
    "    \n",
    "    nstar_lower, nstar_upper = np.quantile(nstar_cv, [CI_LOWER, CI_UPPER])\n",
    "\n",
    "    return ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot the our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_generalizability_and_quantiles(dfy, dfq, logepss, ns_pred, ns_pred_cv, nstar, nstar_upper, nstar_lower, kernel_dir, eps):\n",
    "#     # Create figure and axes\n",
    "#     fig, axes = plt.subplots(2, 1, sharex=\"all\", figsize=(10, 8))\n",
    "#\n",
    "#     # Generalizability plot\n",
    "#     ax = axes[0]\n",
    "#     sns.lineplot(data=dfy, x=\"log(eps)\", y=\"generalizability\", hue=\"n\", ax=ax, palette=palette)\n",
    "#     ax.hlines(ALPHA, ls=\"--\", xmin=np.min(logepss), xmax=np.max(logepss), color=\"black\")\n",
    "#     for n in dfq[\"n\"].unique():\n",
    "#         ax.vlines(dfq.loc[dfq.n==n, \"log(eps)\"].iloc[0], ymin=0, ymax=ALPHA, ls=\":\")\n",
    "#     sns.despine(ax=ax)\n",
    "#\n",
    "#     # Quantiles plot\n",
    "#     ax = axes[1]\n",
    "#     ymax = max(ns_pred)\n",
    "#     sns.lineplot(data=dfq, x=\"log(eps)\", y=\"n\", ax=ax, ls=\"\", marker=\"o\", hue=\"n\", legend=False)\n",
    "#     for n in dfq[\"n\"].unique():\n",
    "#         ax.vlines(dfq.loc[dfq.n==n, \"log(eps)\"].iloc[0], ymin=n, ymax=ymax, ls=\":\")\n",
    "#\n",
    "#     ax.vlines(np.log(eps), ymin=0.1, ymax=ymax, color=\"black\", ls=\"--\")\n",
    "#     sns.lineplot(x=logepss, y=ns_pred, color=\"green\", ls=\"-.\", ax=ax)\n",
    "#\n",
    "#     for it, ns_tmp in enumerate(ns_pred_cv):\n",
    "#         if np.max(ns_tmp) > 1000:\n",
    "#             continue\n",
    "#         sns.lineplot(x=logepss, y=ns_tmp, color=\"green\", ls=\"-.\", alpha=0.5, ax=ax)\n",
    "#\n",
    "#     ax.set_xlabel(r\"$\\log(\\varepsilon)$\")\n",
    "#     ax.set_ylabel(r\"$n$\")\n",
    "#\n",
    "#     # N* Lines\n",
    "#     ax.hlines(nstar, xmin=np.min(logepss), xmax=np.log(eps), ls=\"-\", color=\"red\")\n",
    "#     ax.hlines(nstar_upper, xmin=np.min(logepss), xmax=np.log(eps), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "#     ax.hlines(nstar_lower, xmin=np.min(logepss), xmax=np.log(eps), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "#     ax.set_yscale(\"log\")\n",
    "#     sns.despine(ax=ax)\n",
    "#\n",
    "#     # Finalize and save\n",
    "#     fig.suptitle(f\"Generalizability for $N = {len(ecs):02d}$\\n\"\n",
    "#                  fr\"$n^* (\\alpha={ALPHA}, \\varepsilon={eps:.2f}) = {np.ceil(nstar)}$\" + \"\\n\"\n",
    "#                  f\"${LR_CONFIDENCE}$-confidence interval: $[{np.ceil(nstar_lower)}, {np.ceil(nstar_upper)}]$\")\n",
    "#     plt.tight_layout()\n",
    "#     if FORMAT == \"pdf\" or FORMAT == \"all\":\n",
    "#         plt.savefig(kernel_dir / f\"N={len(ecs):02d}.pdf\")\n",
    "#     if FORMAT == \"png\" or FORMAT == \"all\":\n",
    "#         plt.savefig(kernel_dir / f\"N={len(ecs):02d}.png\")\n",
    "#     plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Configurations:   0%|          | 0/48 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7757ed20921c422388b83b49d967223f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.ioff()\n",
    "\n",
    "for fixed_levels, idxs in tqdm(list(groups.items()), position=0, desc=\"Configurations\", leave=True):\n",
    "    idf = df.loc[idxs].reset_index(drop=True)\n",
    "\n",
    "    if idf.empty:\n",
    "        continue\n",
    "\n",
    "    # fixed levels\n",
    "    factors_dict = {factor: lvl\n",
    "                    for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                    if lvl not in [None, \"_all\"]}\n",
    "    factors_dict.update({factor: idf[factor].unique()[0] for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                         if lvl == \"_all\"})\n",
    "\n",
    "    # -- convert df to rank matrix\n",
    "    rank_matrix = ru.get_rankings_from_df(idf, factors=list(EXPERIMENTAL_FACTORS.keys()), \n",
    "                                            alternatives=ALTERNATIVES,\n",
    "                                            target=TARGET,\n",
    "                                            lower_is_better=False, impute_missing=True)\n",
    "    rank_matrix = rank_matrix.fillna(rank_matrix.max())\n",
    "\n",
    "    # -- get all kernels\n",
    "    kernels = init_kernels(rank_matrix)\n",
    "\n",
    "    # -- set up the ec pool\n",
    "    ec_variable = next((key for key, value in EXPERIMENTAL_FACTORS.items() if value is None), None)\n",
    "    ec_pool = idf[ec_variable].unique()\n",
    "    ecs = np.array([])\n",
    "\n",
    "    # for kernelname, (kernel, kernelargs, epsstar) in tqdm(kernels.items(), position=1, desc='Kernels', leave=False):\n",
    "    for kernelname, (kernel, kernelargs, epsstar, deltastar) in kernels.items():\n",
    "        nstar_dir, gen_dir, quant_dir = create_experiment_directory(kernelname, factors_dict, epsstar)\n",
    "        out = []\n",
    "        # for i in tqdm(range(len(ec_pool) // SAMPLE_SIZE), desc=f'Using {kernelname}', leave=False):\n",
    "        for i in range(len(ec_pool) // SAMPLE_SIZE):\n",
    "\n",
    "            if (i+1)*SAMPLE_SIZE > len(ec_pool):\n",
    "                break\n",
    "\n",
    "            # -- Sample new rankings from ec pool\n",
    "            ecs = sample_ecs(ec_pool, (i+1)*SAMPLE_SIZE)\n",
    "            rankings, nv = compute_rankings(ecs, rank_matrix)\n",
    "\n",
    "            # -- Compute the lower bound\n",
    "            variance, var_lower_bound = compute_variance_and_lower_bound(rankings, n=len(ecs), kbar=1, eps=epsstar, kernel=kernel, kernelargs=kernelargs)\n",
    "\n",
    "            # -- We do not need to compute dfy and dfq again if we have already computed them for another alpha/epsstar\n",
    "            if f\"dfy_{len(ecs)}\" in [x.stem for x in gen_dir.glob(\"*.parquet\")] and f\"dfmmd_{len(ecs)}\" in [x.stem for x in quant_dir.glob(\"*.parquet\")]:\n",
    "                try:\n",
    "                    dfy = pd.read_parquet(gen_dir / f\"dfy_{len(ecs)}.parquet\")\n",
    "                    dfmmd = pd.read_parquet(quant_dir / f\"dfmmd_{len(ecs)}.parquet\")\n",
    "\n",
    "                    dfq = pd.DataFrame(dfmmd.groupby(\"n\")[\"eps\"].quantile(ALPHA)).reset_index()\n",
    "                    dfq[\"log(eps)\"] = np.log(dfq[\"eps\"])\n",
    "                    dfq[\"log(n)\"] = np.log(dfq[\"n\"])\n",
    "\n",
    "                    logepss = dfy[\"log(eps)\"].unique()\n",
    "                except Exception as e:\n",
    "                    print(factors_dict)\n",
    "                    raise e\n",
    "            else:\n",
    "                # -- Compute mmds\n",
    "                mmds = calculate_mmds(rankings, nv, kernel=kernel, kernelargs=kernelargs)\n",
    "                dfmmd = pd.DataFrame(mmds).melt(var_name=\"n\", value_name=\"eps\")\n",
    "\n",
    "                # -- Compute generalizability and quantiles\n",
    "\n",
    "                # - Prepare log(eps) scale\n",
    "                logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
    "\n",
    "                # - Dataframe for generalizability\n",
    "                dfy = create_generalizability_dataframe(mmds, logepss)\n",
    "\n",
    "                # - Dataframe for quantiles\n",
    "                dfq = create_quantiles_dataframe(mmds)\n",
    "\n",
    "            # -- Linear Regression with Cross-Validation\n",
    "            linear_predictors, residuals = perform_linear_regression_with_cv(dfq)\n",
    "\n",
    "            # -- Predictions\n",
    "            ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper = predict_nstar(logepss, linear_predictors, dfq, epsstar)\n",
    "\n",
    "            # -- Plotting\n",
    "            # plot_generalizability_and_quantiles(dfy, dfq, logepss, ns_pred, ns_pred_cv, nstar, nstar_upper, nstar_lower, nstar_dir, epsstar)\n",
    "\n",
    "            # -- Storing\n",
    "            result_dict = {\n",
    "                \"kernel\": kernelname,\n",
    "                \"alpha\": ALPHA,\n",
    "                \"eps\": epsstar,\n",
    "                \"delta\": deltastar,\n",
    "                \"disjoint\": DISJOINT,\n",
    "                \"replace\": REPLACE,\n",
    "                \"N\": len(ecs),\n",
    "                \"nstar\": nstar,\n",
    "                \"nstar_lower\": nstar_lower,\n",
    "                \"nstar_upper\": nstar_upper,\n",
    "                \"variance\": variance,\n",
    "                \"var_lower_bound\": var_lower_bound,\n",
    "            }\n",
    "            result_dict.update(factors_dict)\n",
    "            out.append(result_dict)\n",
    "\n",
    "            dfy.to_parquet(gen_dir / f\"dfy_{len(ecs)}.parquet\")\n",
    "            # dfq.to_parquet(quant_dir / f\"dfq_{len(ecs)}_{ALPHA}.parquet\")\n",
    "            dfmmd.to_parquet(quant_dir / f\"dfmmd_{len(ecs)}.parquet\")\n",
    "\n",
    "        # if FORMAT == \"gif\" or FORMAT == \"all\":\n",
    "        #     images = [iio.imread(image) for image in glob.glob(str(nstar_dir / \"*.png\"))]\n",
    "        #     iio.mimwrite(nstar_dir / f\"nstar.gif\", images, duration=750, loop=0)\n",
    "        # -- Store nstar predictions\n",
    "        out = pd.DataFrame(out)\n",
    "        out.to_parquet(nstar_dir / \"nstar.parquet\")\n",
    "# plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, we collect the results for nstar into a single dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading dataframes:   0%|          | 0/1296 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "391ff81a748b496e99871a1f43f35dee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nstars = [pd.read_parquet(x)\n",
    "             for x in tqdm(list(OUTPUT_DIR.glob(\"**/**/**/nstar.parquet\")), desc=\"Loading dataframes\")]\n",
    "df_nstar = pd.concat(df_nstars).reset_index(drop=True)\n",
    "df_nstar[\"eps\"] = df_nstar[\"eps\"].round(3)\n",
    "\n",
    "fixed_factors = [factor for factor, lvl in EXPERIMENTAL_FACTORS.items() if lvl == \"_all\"]\n",
    "maxN = df_nstar.groupby(fixed_factors)[\"N\"].max()\n",
    "df_nstar = df_nstar.join(maxN, on=fixed_factors, rsuffix=\"max\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to prepere the dataframe for plotting.\n",
    "First of all, we fix epsilon to delta=0.05 and let alpha vary.\n",
    "The corresponding rounded values for epsilon are 0.221 for the Jaccard kernel and 0.224 for the Mallows and Borda kernels.\n",
    "Second, we make sure that N is at the maximum for every combination of levels of not allowed-to-vary factors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's fix the plotting parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", palette=\"flare_r\", context=\"paper\", font=\"times new roman\")\n",
    "\n",
    "preamble = r\"\"\"\n",
    "    \\usepackage{mathptmx}\n",
    "    \\usepackage{amsmath}\n",
    "\"\"\"\n",
    "mpl.use(\"TkAgg\")\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = preamble\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "# pretty names\n",
    "pc = {\"alpha\": r\"$\\alpha^*$\", \"eps\": r\"$\\varepsilon^*$\", \"nstar\": r\"$n^*$\", \"delta\": r\"$\\delta^*$\", \"N\": r\"$N$\", \"nstar_absrel_error\": \"relative error\"}  # pretty columns\n",
    "pk = {\"borda_kernel_idx_OHE\": r\"$\\kappa_b^{\\text{OHE}, 1/n}$\", \"mallows_kernel_nu_auto\": r\"$\\kappa_m^{1/\\binom{n}{2}}$\", \"jaccard_kernel_k_1\": r\"$\\kappa_j^{1}$\"}\n",
    "pk.update({\"borda_kernel_idx_OHE\": \"$g_1$\", \"mallows_kernel_nu_auto\": \"$g_3$\", \"jaccard_kernel_k_1\": \"$g_2$\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now add the worst case scenario theoretical upper bound prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def theoretical_nstar(alphastar, epsstar, kbar=1):\n",
    "    beta1 = -2\n",
    "    beta0 = 2*np.log(np.sqrt(2*kbar) + np.sqrt(-4*kbar * np.log(1-alphastar)))\n",
    "    return np.exp(beta0 + beta1*np.log(epsstar))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Varying $\\alpha$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "dfplot = df_nstar.loc[(df_nstar[\"delta\"] == 0.05) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "dfplot[\"nstar_th\"] = theoretical_nstar(dfplot[pc[\"alpha\"]], dfplot[pc[\"eps\"]], kbar=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Three plots in line"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "\n",
    "plt.close(\"all\")\n",
    "order = np.sort(dfplot[pc[\"alpha\"]].unique())\n",
    "\n",
    "g = sns.FacetGrid(data=dfplot, col=\"kernel\", sharey=True, aspect=1, height=5.5/3)\n",
    "# add theoretical\n",
    "# for ax, k in zip(g.axes.flat, [\"borda\", \"jaccard\", \"mallows\"]):\n",
    "#     alphas = np.linspace(dfplot[pc[\"alpha\"]].min(), dfplot[pc[\"alpha\"]].max(), 100)\n",
    "#     eps = 0.316 if k == \"jaccard\" else 0.312\n",
    "#     nstars_th = theoretical_nstar(alphas, eps)\n",
    "#     sns.lineplot(x=alphas, y=nstars_th, ax=ax, c=\"black\", linewidth=0.5)\n",
    "\n",
    "g.map(sns.boxplot, pc[\"alpha\"], pc[\"nstar\"], width=0.4, showfliers=False, native_scale=True, order=order,\n",
    "      boxprops={\"alpha\": 0.3})\n",
    "g.map(sns.swarmplot, pc[\"alpha\"], pc[\"nstar\"], native_scale=True, order=order, size=0.75)\n",
    "g.map(plt.grid, color=\"grey\", alpha=0.2)\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set(xticks=[0.7, 0.8, 0.9, 0.99])\n",
    "\n",
    "g.tight_layout(pad=0.5)\n",
    "g.savefig(FIGURES_DIR / \"nstar_alpha.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One plot for alpha and delta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5.5, 5.5/2.5), width_ratios=(1, 1), sharey=True)\n",
    "\n",
    "# ----  ALPHA\n",
    "ax = axes[0]\n",
    "dfplot = df_nstar.loc[(df_nstar[\"delta\"] == 0.05) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "dfplot[\"nstar_th\"] = theoretical_nstar(dfplot[pc[\"alpha\"]], dfplot[pc[\"eps\"]], kbar=1)\n",
    "\n",
    "# plot\n",
    "sns.boxplot(dfplot, x=pc[\"alpha\"], y=pc[\"nstar\"], ax=ax, hue=\"kernel\", showfliers=False, palette=\"cubehelix\",\n",
    "            dodge=True, native_scale=True, fill=False, legend=False,\n",
    "            width=0.75, boxprops={\"linewidth\": 1.2}, gap=0.25)\n",
    "# ax.set(xticks=[0.7, 0.8, 0.9, 0.99])\n",
    "ax.grid(color=\"grey\", alpha=0.2)\n",
    "\n",
    "# theoretical bound\n",
    "alphas = np.linspace(dfplot[pc[\"alpha\"]].min(), dfplot[pc[\"alpha\"]].max(), 100)\n",
    "eps = 0.316\n",
    "nstars_th = theoretical_nstar(alphas, eps)\n",
    "sns.lineplot(x=alphas, y=nstars_th, ax=ax, c=\"black\", linewidth=0.5)\n",
    "\n",
    "# ----  DELTA\n",
    "ax = axes[1]\n",
    "dfplot = df_nstar.loc[(df_nstar[\"alpha\"] == 0.95) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "\n",
    "# plot\n",
    "sns.boxplot(dfplot, x=pc[\"delta\"], y=pc[\"nstar\"], ax=ax, hue=\"kernel\", showfliers=False, palette=\"cubehelix\",\n",
    "            dodge=True, native_scale=True, fill=False, legend=True,\n",
    "            width=0.75, boxprops={\"linewidth\": 1.2}, gap=0.25)\n",
    "# ax.set(xticks=[0.01, 0.1, 0.2, 0.3])\n",
    "ax.grid(color=\"grey\", alpha=0.2)\n",
    "\n",
    "a = 0.95\n",
    "epss = np.linspace(dfplot[pc[\"eps\"]].min(), dfplot[pc[\"eps\"]].max(), 100)\n",
    "nstars_th = theoretical_nstar(a, epss)\n",
    "sns.lineplot(x=epss, y=nstars_th, ax=ax, c=\"black\", linewidth=0.5)\n",
    "\n",
    "# nice legend\n",
    "ax.legend(*ax.get_legend_handles_labels()).get_frame().set_edgecolor(\"w\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "sns.despine(right=True, top=True)\n",
    "plt.tight_layout(pad=.5)\n",
    "plt.subplots_adjust(wspace=.12)\n",
    "# plt.savefig(FIGURES_DIR / \"encoders_nstar_alpha_delta.pdf\")\n",
    "plt.savefig(FIGURES_DIR / \"encoders_nstar_alpha_delta_upper.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legends\n",
      "legend\n"
     ]
    }
   ],
   "source": [
    "for x in fig.__dir__():\n",
    "    if \"legend\" in x:\n",
    "        print(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Varying $\\delta$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "dfplot = df_nstar.loc[(df_nstar[\"alpha\"] == 0.95) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "\n",
    "plt.close(\"all\")\n",
    "order = np.sort(dfplot[pc[\"delta\"]].unique())\n",
    "\n",
    "g = sns.FacetGrid(data=dfplot, col=\"kernel\", sharey=True, aspect=1, height=5.5/3)\n",
    "\n",
    "# # add theoretical\n",
    "# for ax, k in zip(g.axes.flat, [\"borda\", \"jaccard\", \"mallows\"]):\n",
    "#     a = 0.95\n",
    "#     epss = np.linspace(dfplot[pc[\"eps\"]].min(), dfplot[pc[\"eps\"]].max(), 100)\n",
    "#     nstars_th = theoretical_nstar(a, epss)\n",
    "#     sns.lineplot(x=epss, y=nstars_th, ax=ax, c=\"black\", linewidth=0.5)\n",
    "\n",
    "g.map(sns.boxplot, pc[\"delta\"], pc[\"nstar\"], width=0.4, showfliers=False, native_scale=True, order=order,\n",
    "      boxprops={\"alpha\": 0.3})\n",
    "g.map(sns.swarmplot, pc[\"delta\"], pc[\"nstar\"], native_scale=True, order=order, size=0.75)\n",
    "g.map(plt.grid, color=\"grey\", alpha=0.2)\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "# g.set(xticks=[0.2, 0.4, 0.6, 0.8], yscale=\"log\")\n",
    "g.set(xticks=[0.01, 0.1, 0.2, 0.3])\n",
    "\n",
    "g.tight_layout(pad=0.5)\n",
    "g.savefig(FIGURES_DIR / \"nstar_delta.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the weird combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_ = dfplot.copy()\n",
    "# df_ = df_.loc[df_[\"kernel\"] == pk[\"jaccard_kernel_k_1\"]]\n",
    "# df_ = df_.loc[df_[pc[\"eps\"]] == df_[pc[\"eps\"]].max()]\n",
    "#\n",
    "# df_.loc[df_[pc[\"nstar\"]] == df_[pc[\"nstar\"]].min()][[pc[\"nstar\"], \"model\", \"tuning\", \"scoring\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extreme predictions for nstar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "                  kernel  alpha    eps  delta  disjoint  replace   N  \\\n38    jaccard_kernel_k_1   0.95  0.316   0.05      True    False  30   \n4763  jaccard_kernel_k_1   0.95  0.316   0.05      True    False  30   \n\n          nstar  nstar_lower  nstar_upper  variance  var_lower_bound model  \\\n38    27.295829    26.825952    27.635111  0.929974         0.679600   DTC   \n4763  33.858101    33.390801    34.414649  0.909473         0.685465   SVC   \n\n     tuning scoring  Nmax  \n38     full     ACC    30  \n4763   full    BACC    30  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kernel</th>\n      <th>alpha</th>\n      <th>eps</th>\n      <th>delta</th>\n      <th>disjoint</th>\n      <th>replace</th>\n      <th>N</th>\n      <th>nstar</th>\n      <th>nstar_lower</th>\n      <th>nstar_upper</th>\n      <th>variance</th>\n      <th>var_lower_bound</th>\n      <th>model</th>\n      <th>tuning</th>\n      <th>scoring</th>\n      <th>Nmax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>38</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>30</td>\n      <td>27.295829</td>\n      <td>26.825952</td>\n      <td>27.635111</td>\n      <td>0.929974</td>\n      <td>0.679600</td>\n      <td>DTC</td>\n      <td>full</td>\n      <td>ACC</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4763</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>30</td>\n      <td>33.858101</td>\n      <td>33.390801</td>\n      <td>34.414649</td>\n      <td>0.909473</td>\n      <td>0.685465</td>\n      <td>SVC</td>\n      <td>full</td>\n      <td>BACC</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = \"jaccard_kernel_k_1\"\n",
    "\n",
    "df_ = df_nstar.query(\"kernel==@kernel and alpha==0.95 and delta==0.05\")\n",
    "df_ = df_.loc[df_[\"N\"] == df_[\"Nmax\"]]\n",
    "df_.loc[(df_[\"nstar\"] == df_[\"nstar\"].min()) | (df_[\"nstar\"] == df_[\"nstar\"].max())]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nstar prediction from N\n",
    "\n",
    "See how the prediction changes with N"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "true_nstar = df_nstar.loc[df_nstar[\"N\"] == df_nstar[\"Nmax\"]].drop(columns=[\"N\", \"Nmax\"])\n",
    "keys = [\"kernel\", \"alpha\", \"eps\", \"model\", \"tuning\", \"scoring\"]\n",
    "df_ = pd.merge(df_nstar, true_nstar, left_on=keys, right_on=keys, suffixes=(\"\", \"_true\"))[keys + [\"nstar\", \"nstar_true\", \"N\", \"Nmax\"]]\n",
    "df_[\"nstar_error\"] = df_[\"nstar\"] - df_[\"nstar_true\"]\n",
    "df_[\"nstar_relative_error\"] = (df_[\"nstar\"] - df_[\"nstar_true\"]) / df_[\"nstar_true\"]\n",
    "df_[\"nstar_absolute_error\"] = np.abs(df_[\"nstar\"] - df_[\"nstar_true\"])\n",
    "df_[\"nstar_absrel_error\"] = np.abs(df_[\"nstar\"] - df_[\"nstar_true\"]) / df_[\"nstar_true\"]\n",
    "# df_ = df_.loc[df_[\"N\"] != df_[\"Nmax\"]]\n",
    "print(len(df_.groupby([\"model\", \"tuning\", \"scoring\"]).groups))\n",
    "df_ = df_.query(\"Nmax == 50\")\n",
    "print(len(df_.groupby([\"model\", \"tuning\", \"scoring\"]).groups))\n",
    "\n",
    "dfplot = df_.copy().query(\"N < Nmax\").rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "\n",
    "y = pc[\"nstar_absrel_error\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5.5/2, 2))\n",
    "\n",
    "sns.boxplot(dfplot, x=pc[\"N\"], y=y, showfliers=False, fliersize=0.3, hue=\"kernel\", palette=\"cubehelix\", ax=ax, legend=False, linewidth=1.2, fill=False, gap=0.2)\n",
    "\n",
    "ax.grid(color=\"grey\", alpha=.2)\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8])\n",
    "ax.set_title(\"(a) Categorical encoders\")\n",
    "\n",
    "# nice legend\n",
    "ax.legend(*ax.get_legend_handles_labels()).get_frame().set_edgecolor(\"w\")\n",
    "sns.despine()\n",
    "plt.tight_layout(pad=.5)\n",
    "\n",
    "plt.savefig(FIGURES_DIR / \"encoders_nstar_absrel_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "23"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of design factor combinations left\n",
    "len(df_.groupby([\"model\", \"tuning\", \"scoring\"]).groups)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intuitive explanation of generalizability\n",
    "\n",
    "Plot generalizability as function of 2n, for some fixed epsilon.\n",
    "On the same plot, plot the average kernel within the 2n sampled experimental conditions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DT'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'DT'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[165], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m rf \u001B[38;5;241m=\u001B[39m ru\u001B[38;5;241m.\u001B[39mget_rankings_from_df(df\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m      2\u001B[0m                              factors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(EXPERIMENTAL_FACTORS\u001B[38;5;241m.\u001B[39mkeys()),\n\u001B[0;32m      3\u001B[0m                              alternatives\u001B[38;5;241m=\u001B[39mALTERNATIVES,\n\u001B[0;32m      4\u001B[0m                              target\u001B[38;5;241m=\u001B[39mTARGET,\n\u001B[0;32m      5\u001B[0m                              lower_is_better\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, impute_missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mrf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDT\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1189\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m   1190\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001B[1;32m-> 1191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1429\u001B[0m \u001B[38;5;66;03m# fall thru to straight lookup\u001B[39;00m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_key(key, axis)\n\u001B[1;32m-> 1431\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_label\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001B[0m, in \u001B[0;36m_LocIndexer._get_label\u001B[1;34m(self, label, axis)\u001B[0m\n\u001B[0;32m   1379\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_label\u001B[39m(\u001B[38;5;28mself\u001B[39m, label, axis: AxisInt):\n\u001B[0;32m   1380\u001B[0m     \u001B[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001B[39;00m\n\u001B[1;32m-> 1381\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mxs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:4287\u001B[0m, in \u001B[0;36mNDFrame.xs\u001B[1;34m(self, key, axis, level, drop_level)\u001B[0m\n\u001B[0;32m   4285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4286\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_level:\n\u001B[1;32m-> 4287\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   4288\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m   4289\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4101\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_single_key:\n\u001B[0;32m   4100\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 4101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_multilevel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4102\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[0;32m   4103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4159\u001B[0m, in \u001B[0;36mDataFrame._getitem_multilevel\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_getitem_multilevel\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m   4158\u001B[0m     \u001B[38;5;66;03m# self.columns is a MultiIndex\u001B[39;00m\n\u001B[1;32m-> 4159\u001B[0m     loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loc, (\u001B[38;5;28mslice\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray)):\n\u001B[0;32m   4161\u001B[0m         new_columns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns[loc]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3040\u001B[0m, in \u001B[0;36mMultiIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3037\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mask\n\u001B[0;32m   3039\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m-> 3040\u001B[0m     loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_level_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3041\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _maybe_to_slice(loc)\n\u001B[0;32m   3043\u001B[0m keylen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(key)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3391\u001B[0m, in \u001B[0;36mMultiIndex._get_level_indexer\u001B[1;34m(self, key, level, indexer)\u001B[0m\n\u001B[0;32m   3388\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mslice\u001B[39m(i, j, step)\n\u001B[0;32m   3390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3391\u001B[0m     idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_loc_single_level_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlevel_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m level \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lexsort_depth \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   3394\u001B[0m         \u001B[38;5;66;03m# Desired level is not sorted\u001B[39;00m\n\u001B[0;32m   3395\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mslice\u001B[39m):\n\u001B[0;32m   3396\u001B[0m             \u001B[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:2980\u001B[0m, in \u001B[0;36mMultiIndex._get_loc_single_level_index\u001B[1;34m(self, level_index, key)\u001B[0m\n\u001B[0;32m   2978\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2979\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2980\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlevel_index\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\genexpy - local\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'DT'"
     ]
    }
   ],
   "source": [
    "rf = ru.get_rankings_from_df(df.reset_index(drop=True),\n",
    "                             factors=list(EXPERIMENTAL_FACTORS.keys()),\n",
    "                             alternatives=ALTERNATIVES,\n",
    "                             target=TARGET,\n",
    "                             lower_is_better=False, impute_missing=False)\n",
    "\n",
    "rf.loc(axis=1)[\"DT\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
