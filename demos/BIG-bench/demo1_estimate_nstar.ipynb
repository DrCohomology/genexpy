{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many subtasks are enough?\n",
    "\n",
    "As explained in the paper, given a desired generalizability $\\alpha^*$ and a similarity threshold between rankings $\\delta^*$, we show how to estimate the number of subtasks required to obtain generalizable results for a given task. \n",
    "The procedure goes as follows: \n",
    "1. Load the experimental results. \n",
    "2. Query the results for a combination of design and held-constant factors.\n",
    "3. Sample without repetition $N$ levels of the allowed-to-vary factor (`subtask_description`) and query the results accordingly. \n",
    "4. Rank the alternatives according to the target column. \n",
    "5. Estimate the $\\alpha^*$-quantile of MMD for all $n \\leq N$, call it $\\varepsilon^{\\alpha^*}_n$.\n",
    "6. Fit a linear model $\\log(n) = \\beta_1 \\log(\\varepsilon^{\\alpha^*}_n) + \\beta_0$.\n",
    "7. Estimate $n^* = \\exp(\\beta_1 \\log(\\varepsilon(\\delta^*)) + \\beta_0)$, where $\\varepsilon(\\delta^*)$ is a threshold on MMD.\n",
    "We perform most of these operations within the main loop, iterating over the combinations of design factors. \n",
    "\n",
    "We can investigate the behavior of generalizability and $n^*$ for different $\\alpha^*$ and $\\delta^*$ by changing them in `config.yaml` and re-running the notebook. \n",
    "We analyze this in `demo2_plots.ipynb`. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and configuration"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:51.435932Z",
     "start_time": "2024-06-04T08:39:51.429969Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from genexpy import lower_bounds as gu\n",
    "from genexpy import kernels as ku\n",
    "from genexpy import rankings_utils as ru\n",
    "from genexpy import mmd as mmd"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Next, we load the parameters from the configuration file. "
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:51.553703Z",
     "start_time": "2024-06-04T08:39:51.539456Z"
    }
   },
   "source": [
    "with open(\"config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "OUTPUT_DIR = Path(config['paths']['output_dir'])\n",
    "FIGURES_DIR = Path(config['paths']['figures_dir'])\n",
    "\n",
    "SEED = config['parameters']['seed']\n",
    "RNG = np.random.default_rng(SEED)\n",
    "ALPHA = config['parameters']['alpha']\n",
    "LR_CONFIDENCE = config['parameters']['lr_confidence']\n",
    "CI_LOWER = (1 - LR_CONFIDENCE) / 2\n",
    "CI_UPPER = LR_CONFIDENCE + CI_LOWER\n",
    "\n",
    "DATASET = Path(config['data']['dataset_path'])\n",
    "EXPERIMENTAL_FACTORS = config['data']['experimental_factors']\n",
    "TARGET = config['data']['target']\n",
    "ALTERNATIVES = config['data']['alternatives']\n",
    "\n",
    "SAMPLE_SIZE = config['sampling']['sample_size']\n",
    "DISJOINT = config['sampling']['disjoint']\n",
    "REPLACE = config['sampling']['replace']"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we initialize the kernels, storing them in a dictionary together with heir parameters and the specified $\\delta^*$ (one for each kernel). "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:51.682976Z",
     "start_time": "2024-06-04T08:39:51.675790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "KERNELS = {}\n",
    "for kernel_config in config['kernels']:\n",
    "    kernel_func = getattr(ku, kernel_config['kernel'], None)\n",
    "    \n",
    "    if kernel_func:\n",
    "        delta = kernel_config['delta']  # to get epsilon\n",
    "        match kernel_config['kernel']:\n",
    "            case \"mallows_kernel\":\n",
    "                eps = np.sqrt(2 * (1 - np.exp(-delta)))  # assumes nu = 1/binom(n, 2)\n",
    "            case \"jaccard_kernel\":\n",
    "                eps = np.sqrt(2 * (1 - (1-delta)))\n",
    "            case \"borda_kernel\":\n",
    "                eps = np.sqrt(2 * (1 - np.exp(-delta)))   # assumes nu = 1/n\n",
    "            case _ :\n",
    "                raise ValueError(f\"The kernel {kernel_config['kernel']} must be either the Jaccard, Mallows, or Borda kernel.\")  \n",
    "\n",
    "        for param_key, param_values in kernel_config['params'].items():\n",
    "            if isinstance(param_values, list):\n",
    "                for value in param_values:\n",
    "                    params = {param_key: value}\n",
    "                    kernel_name = f\"{kernel_config['kernel']}_{param_key}_{value}\"\n",
    "                    KERNELS[kernel_name] = (kernel_func, params, eps, delta)\n",
    "            else:\n",
    "                params = {param_key: param_values}\n",
    "                kernel_name = f\"{kernel_config['kernel']}_{param_key}_{param_values}\"\n",
    "                KERNELS[kernel_name] = (kernel_func, params, eps, delta)\n",
    "    else:\n",
    "        print(f\"Kernel function '{kernel_config['kernel']}' not found in module 'kernels'.\")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "First, we load the dataset of results and perform some preliminary checks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:52.468755Z",
     "start_time": "2024-06-04T08:39:51.754537Z"
    }
   },
   "source": [
    "import warnings\n",
    "if DATASET.suffix == '.parquet':\n",
    "    df = pd.read_parquet(DATASET)\n",
    "elif DATASET.suffix == '.csv':\n",
    "    df = pd.read_csv(DATASET)\n",
    "else:\n",
    "    raise Exception(\"Please use a Parquet or CSV file as the format of your data\")\n",
    "\n",
    "# Keep the preferred scoring only\n",
    "df = df.query(\"score_key == preferred_score\").drop(columns=[\"score_key\", \"preferred_score\"])\n",
    "\n",
    "# Check whether exactly one of the experimental factors is None \n",
    "assert sum(value is None for value in EXPERIMENTAL_FACTORS.values()) == 1, \"Exactly one experimental factor must be set to null in config.yaml.\"\n",
    "\n",
    "# Check whether the factors listed in the config coincide with the columns of df\n",
    "columns_to_check = set(EXPERIMENTAL_FACTORS.keys()).union({TARGET, ALTERNATIVES})\n",
    "if not_in_df := columns_to_check - set(df.columns):\n",
    "    raise ValueError(f\"The following columns are missing from the dataframe: {not_in_df}\")\n",
    "if not_in_config:= set(df.columns) - columns_to_check:\n",
    "    warnings.warn(f\"The following columns in the dataframe are not required: {not_in_config}\", stacklevel=2)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Second, we keep the tasks with results for at least $80\\%$ of the LLMs, and the LLMs with results on at least $80\\%$ of the remaining tasks."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.190974Z",
     "start_time": "2024-06-04T08:39:52.469596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf = ru.get_rankings_from_df(df.reset_index(drop=True),\n",
    "                             factors=list(EXPERIMENTAL_FACTORS.keys()),\n",
    "                             alternatives=ALTERNATIVES,\n",
    "                             target=TARGET,\n",
    "                             lower_is_better=False, impute_missing=False)\n",
    "\n",
    "tol = 0.2\n",
    "rf = rf.loc[:, rf.isna().sum(axis=0) <= rf.shape[0] * tol]\n",
    "rf = rf.loc[rf.isna().sum(axis=1) <= rf.shape[1] * tol, :]\n",
    "df = df.loc[df.set_index(list(EXPERIMENTAL_FACTORS.keys())).index.isin(rf.columns)]"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Finally, we query the dataset for the specified levels of the held-constant factors and get all the combinations of levels of design factors. "
  },
  {
   "cell_type": "code",
   "source": [
    "# query fot the held-constant factors\n",
    "try:\n",
    "    query_string = \" and \".join(f\"{factor} == '{lvl}'\" if isinstance(lvl, str) else f\"{factor} == {lvl}\"\n",
    "                                for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                                if lvl not in [None, \"_all\"])\n",
    "    df = df.query(query_string)\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "# get the combinations of levels of allowed-to-vary-factors\n",
    "try:\n",
    "    groups = df.groupby([factor for factor, lvl in EXPERIMENTAL_FACTORS.items() if lvl == \"_all\"]).groups\n",
    "except ValueError:\n",
    "    groups = {\"None\": df.index}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.370995Z",
     "start_time": "2024-06-04T08:39:58.190974Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "We now define all the functions we will need to estimate $n^*$ in the main loop. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We start by creating the necessary directories."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.377205Z",
     "start_time": "2024-06-04T08:39:58.373014Z"
    }
   },
   "source": [
    "def create_experiment_directory(kernel_name, factors, delta):\n",
    "    exp0_dir = OUTPUT_DIR / \"_\".join([f\"{key}={value}\" for key, value in factors.items() if value is not None])\n",
    "    exp1_dir = exp0_dir / f\"{kernel_name}\"\n",
    "    exp21_dir = exp1_dir / f\"nstar_N_ALPHA={ALPHA}_delta={delta}_ci={LR_CONFIDENCE}\"\n",
    "    exp21_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exp22_dir = exp1_dir / \"computed_generalizability\"\n",
    "    exp22_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exp23_dir = exp1_dir / \"computed_quantiles\"\n",
    "    exp23_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return exp21_dir, exp22_dir, exp23_dir"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Then, we sample some levels from allowed-to-vary factor, query the dataset of results, and compute the rankings."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.385229Z",
     "start_time": "2024-06-04T08:39:58.377205Z"
    }
   },
   "source": [
    "def sample_ecs(ec_pool, sample_size):\n",
    "    assert sample_size <= len(ec_pool), f\"Sample size {sample_size} is larger than |ec_pool| = {len(ec_pool)}\"\n",
    "    return RNG.choice(ec_pool, sample_size, replace=False)\n",
    "\n",
    "def compute_rankings(ecs, rank_matrix):\n",
    "    rm_ = rank_matrix.loc[:, ecs]\n",
    "    na, nv = rm_.shape\n",
    "    \n",
    "    # Generate rankings from the data\n",
    "    rankings = ru.SampleAM.from_rank_function_dataframe(rm_)\n",
    "    \n",
    "    return rankings, nv"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the rankings, we compute the variance and a lower bound on generalizability we build from [1] and then compute the Maximum ean Discrepancy (MMD) for pairs of samples of varying sizes $n$. \n",
    "We will use it to estimate the $\\alpha^*$-quantile $\\varepsilon^{\\alpha^*}_n$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.392650Z",
     "start_time": "2024-06-04T08:39:58.385229Z"
    }
   },
   "source": [
    "def sample_mmd(rankings, nv, kernel, kernelargs):\n",
    "    mmds = {\n",
    "        n: mmd.subsample_mmd_distribution(\n",
    "            rankings, subsample_size=n, rep=100, use_rv=True, use_key=False,\n",
    "            seed=SEED, disjoint=DISJOINT, replace=REPLACE, kernel=kernel, **kernelargs\n",
    "        )\n",
    "        for n in range(2, min(nv // 2 + 1, 50))  # limited to 50 \n",
    "    }\n",
    "    return mmds"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "With the samples of MMD, we calculate the generalizability and $\\alpha^*$-quantile for each $n$."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.399727Z",
     "start_time": "2024-06-04T08:39:58.392650Z"
    }
   },
   "source": [
    "def create_generalizability_dataframe(mmds, logepss):\n",
    "    ys = {n: [mmd.generalizability(mmde, np.exp(logeps)) for logeps in logepss] for n, mmde in mmds.items()}\n",
    "    dfy = pd.DataFrame(ys, index=logepss).reset_index().melt(id_vars='index', var_name='n', value_name='generalizability')\n",
    "    dfy.rename(columns={'index': 'log(eps)'}, inplace=True)\n",
    "    dfy['n'] = dfy['n'].astype(int)\n",
    "    return dfy\n",
    "\n",
    "def create_quantiles_dataframe(mmds):\n",
    "    qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
    "    dfq = pd.DataFrame(list(qs.items()), columns=['n', 'log(eps)'])\n",
    "    dfq['log(n)'] = np.log(dfq['n'])\n",
    "    return dfq"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Finally, we fit linear regression $\\log(n) = \\beta_1\\log(\\varepsilon^{\\alpha^*}_n) = \\beta_0$ and use it to predit $n^*$ with a leave-one-out confidence interval."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:39:58.408874Z",
     "start_time": "2024-06-04T08:39:58.399727Z"
    }
   },
   "source": [
    "def perform_linear_regression_with_cv(dfq):\n",
    "    # Extracting features and target from DataFrame\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    cv = KFold(n_splits=len(y))\n",
    "    residuals, linear_predictors = [], []\n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        lr = LinearRegression().fit(X[train_index], y[train_index])\n",
    "        predicted = lr.predict(X[test_index])\n",
    "        residuals.extend(y[test_index] - predicted)\n",
    "\n",
    "        linear_predictors.append(lr)\n",
    "\n",
    "    return linear_predictors, residuals\n",
    "\n",
    "def predict_nstar(logepss, linear_predictors, dfq, eps):\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    ns_pred_cv = [np.exp(lr.predict(logepss.reshape(-1, 1)).reshape(-1)) for lr in linear_predictors]\n",
    "    ns_pred = np.exp(LinearRegression().fit(X, y).predict(logepss.reshape(-1, 1)).reshape(-1))\n",
    "    nstar_cv = [pred[np.argmax(logepss > np.log(eps))] for pred in ns_pred_cv if not np.all(pred == 0)]\n",
    "    nstar = ns_pred[np.argmax(logepss > np.log(eps))]\n",
    "    nstar_lower, nstar_upper = np.quantile(nstar_cv, [CI_LOWER, CI_UPPER])\n",
    "    return ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Main loop\n",
    "\n",
    "We are finally ready to run the main loop on the combinations of design factors.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T08:52:37.084052Z",
     "start_time": "2024-06-04T08:39:58.408874Z"
    }
   },
   "source": [
    "for fixed_levels, idxs in tqdm(list(groups.items()), position=0, desc=\"Configurations\", leave=True):\n",
    "    \n",
    "    # Query the results for the fixed-levels\n",
    "    idf = df.loc[idxs].reset_index(drop=True)\n",
    "    if idf.empty:\n",
    "        continue\n",
    "\n",
    "    # Current levels of design and held-constant factor\n",
    "    factors_dict = {factor: lvl\n",
    "                    for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                    if lvl not in [None, \"_all\"]}\n",
    "    factors_dict.update({factor: idf[factor].unique()[0] for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                         if lvl == \"_all\"})\n",
    "\n",
    "    # Rank the alternatives  \n",
    "    rank_matrix = ru.get_rankings_from_df(idf, factors=list(EXPERIMENTAL_FACTORS.keys()), \n",
    "                                            alternatives=ALTERNATIVES,\n",
    "                                            target=TARGET,\n",
    "                                            lower_is_better=False, impute_missing=True)\n",
    "    # Impute the missing values\n",
    "    rank_matrix = rank_matrix.fillna(rank_matrix.max())\n",
    "\n",
    "    # Get the pool of experimental conditions to sample from (combinations of allowed-to-vary factors)\n",
    "    atv_factor = next((key for key, value in EXPERIMENTAL_FACTORS.items() if value is None), None)\n",
    "    ec_pool = idf[atv_factor].unique()\n",
    "    ecs = np.array([])\n",
    "\n",
    "    # Loop over the kernels\n",
    "    for kernelname, (kernel, kernelargs, epsstar, deltastar) in KERNELS.items():\n",
    "        \n",
    "        # Update the parameters of the Borda kernel with the index of the alternative in the dataframe of rankings (might vary)\n",
    "        if \"borda\" in kernelname:\n",
    "            kernelargs.update({\"idx\": rank_matrix.index.get_loc(kernelargs[\"alternative\"])})\n",
    "        \n",
    "        # Iteratively sample from ec_pool increasing the sample size at every iteration\n",
    "        # The sampled experimental conditions are used to approximate the distribution of true results\n",
    "        nstar_dir, gen_dir, quant_dir = create_experiment_directory(kernelname, factors_dict, epsstar)\n",
    "        out = []\n",
    "        # for i in tqdm(range(len(ec_pool) // SAMPLE_SIZE), desc=f'Using {kernelname}', leave=False):\n",
    "        for i in range(len(ec_pool) // SAMPLE_SIZE):\n",
    "\n",
    "            if (i+1)*SAMPLE_SIZE > len(ec_pool):\n",
    "                break\n",
    "            if (i+1)*SAMPLE_SIZE > 50:\n",
    "                break\n",
    "\n",
    "            # Sample new experimental conditions from ec_pool and get their rankings\n",
    "            ecs = sample_ecs(ec_pool, (i+1)*SAMPLE_SIZE)\n",
    "            rankings, nv = compute_rankings(ecs, rank_matrix)\n",
    "\n",
    "            # Compute the variance of the experimental results and get the lower bound (conservative estimate) on the distance between the sampled mean embedding and the true mean embedding in the RKHS\n",
    "            variance = ku.var(rankings, use_rv=True, kernel=kernel, **kernelargs)\n",
    "            var_lower_bound = gu.sample_mean_embedding_lowerbound(eps=epsstar, n=len(ecs), kbar=1, v=variance)\n",
    "\n",
    "            # We do not need to compute dfy and dfq again if we have already computed them for another (alphastar, deltastar)\n",
    "            if f\"dfy_{len(ecs)}\" in [x.stem for x in gen_dir.glob(\"*.parquet\")] and f\"dfmmd_{len(ecs)}\" in [x.stem for x in quant_dir.glob(\"*.parquet\")]:\n",
    "                try:\n",
    "                    dfy = pd.read_parquet(gen_dir / f\"dfy_{len(ecs)}.parquet\")\n",
    "                    dfmmd = pd.read_parquet(quant_dir / f\"dfmmd_{len(ecs)}.parquet\")\n",
    "\n",
    "                    dfq = pd.DataFrame(dfmmd.groupby(\"n\")[\"eps\"].quantile(ALPHA)).reset_index()\n",
    "                    dfq[\"log(eps)\"] = np.log(dfq[\"eps\"])\n",
    "                    dfq[\"log(n)\"] = np.log(dfq[\"n\"])\n",
    "\n",
    "                    logepss = dfy[\"log(eps)\"].unique()\n",
    "                except Exception as e:\n",
    "                    print(\"Exception thrown for the following combination of experimental conditions: \", factors_dict)\n",
    "                    raise e\n",
    "            else:\n",
    "                # Sample the distribution of MMD for varying sizes  \n",
    "                mmds = sample_mmd(rankings, nv, kernel=kernel, kernelargs=kernelargs)\n",
    "                dfmmd = pd.DataFrame(mmds).melt(var_name=\"n\", value_name=\"eps\")\n",
    "\n",
    "                # Compute generalizability and quantiles\n",
    "                logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000) \n",
    "                dfy = create_generalizability_dataframe(mmds, logepss)\n",
    "                dfq = create_quantiles_dataframe(mmds)\n",
    "\n",
    "            # Linear regression with cross-validation\n",
    "            # If the results are singular (always the same ranking), we output nstar = 1\n",
    "            try:\n",
    "                linear_predictors, residuals = perform_linear_regression_with_cv(dfq)\n",
    "                # -- Predictions\n",
    "                ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper = predict_nstar(logepss, linear_predictors, dfq, epsstar)\n",
    "                singular = False\n",
    "            except ValueError:\n",
    "                nstar = nstar_lower = nstar_upper = 1\n",
    "                singular = True\n",
    "\n",
    "            # Store results\n",
    "            result_dict = {\n",
    "                \"kernel\": kernelname,\n",
    "                \"alpha\": ALPHA,\n",
    "                \"eps\": epsstar,\n",
    "                \"delta\": deltastar,\n",
    "                \"disjoint\": DISJOINT,\n",
    "                \"replace\": REPLACE,\n",
    "                \"N\": len(ecs),\n",
    "                \"nstar\": nstar,\n",
    "                \"nstar_lower\": nstar_lower,\n",
    "                \"nstar_upper\": nstar_upper,\n",
    "                \"variance\": variance,\n",
    "                \"var_lower_bound\": var_lower_bound,\n",
    "                \"singular\": singular\n",
    "            }\n",
    "            result_dict.update(factors_dict)\n",
    "            out.append(result_dict)\n",
    "\n",
    "            dfy.to_parquet(gen_dir / f\"dfy_{len(ecs)}.parquet\")\n",
    "            # dfq.to_parquet(quant_dir / f\"dfq_{len(ecs)}_{ALPHA}.parquet\")  # not needed \n",
    "            dfmmd.to_parquet(quant_dir / f\"dfmmd_{len(ecs)}.parquet\")\n",
    "\n",
    "        # Store nstar predictions\n",
    "        out = pd.DataFrame(out)\n",
    "        out.to_parquet(nstar_dir / \"nstar.parquet\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configurations:   0%|          | 0/463 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecfa83e63c014f119128f0b881ca6d22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\1050841178.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
      "C:\\Users\\federicom\\Documents\\GitHub\\genexpy\\venv\\Lib\\site-packages\\numpy\\core\\function_base.py:158: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\829304923.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\1050841178.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
      "C:\\Users\\federicom\\Documents\\GitHub\\genexpy\\venv\\Lib\\site-packages\\numpy\\core\\function_base.py:158: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\829304923.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\1050841178.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
      "C:\\Users\\federicom\\Documents\\GitHub\\genexpy\\venv\\Lib\\site-packages\\numpy\\core\\function_base.py:158: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\829304923.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\1050841178.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
      "C:\\Users\\federicom\\Documents\\GitHub\\genexpy\\venv\\Lib\\site-packages\\numpy\\core\\function_base.py:158: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\federicom\\AppData\\Local\\Temp\\ipykernel_30772\\829304923.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[1] Wolfer, Geoffrey, and Pierre Alquier. *Variance-aware estimation of kernel mean embedding.* arXiv preprint arXiv:2210.06672 (2022)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
