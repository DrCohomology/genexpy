{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of core features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports and some plot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio.v2 as iio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from numpy.random import Generator\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from genexpy import lower_bounds as gu\n",
    "from genexpy import kernels as ku\n",
    "from genexpy import probability_distributions as prob\n",
    "from genexpy import rankings_utils as ru\n",
    "from genexpy import mmd as mmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the hyperparameters from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "OUTPUT_DIR = Path(config['paths']['output_dir'])\n",
    "FIGURES_DIR = Path(config['paths']['figures_dir'])\n",
    "\n",
    "FORMAT = config['format']['output']\n",
    "\n",
    "SEED = config['parameters']['seed']\n",
    "RNG = np.random.default_rng(SEED)\n",
    "ALPHA = config['parameters']['alpha']\n",
    "LR_CONFIDENCE = config['parameters']['lr_confidence']\n",
    "CI_LOWER = (1 - LR_CONFIDENCE) / 2\n",
    "CI_UPPER = LR_CONFIDENCE + CI_LOWER\n",
    "\n",
    "DATASET = Path(config['data']['dataset_path'])\n",
    "EXPERIMENTAL_FACTORS = config['data']['experimental_factors']\n",
    "TARGET = config['data']['target']\n",
    "ALTERNATIVES = config['data']['alternatives']\n",
    "\n",
    "SAMPLE_SIZE = config['sampling']['sample_size']\n",
    "DISJOINT = config['sampling']['disjoint']\n",
    "REPLACE = config['sampling']['replace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the dataset into a pandas dataframe and we check that only one experimental factor is set to `None`, indicating that it is allowed to vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "if DATASET.suffix == '.parquet':\n",
    "    df = pd.read_parquet(DATASET).query(\"preferred_score == score_key\").drop(columns=[\"preferred_score\", \"score_key\"])\n",
    "elif DATASET.suffix == '.csv':\n",
    "    df = pd.read_csv(DATASET).query(\"preferred_score == score_key\").drop(columns=[\"preferred_score\", \"score_key\"])\n",
    "else:\n",
    "    raise Exception(\"Please use a Parquet or CSV file as the format of your data\")\n",
    "\n",
    "# preprocessing: remove underepresented models and tasks\n",
    "rf = ru.get_rankings_from_df(df.reset_index(drop=True),\n",
    "                             factors=list(EXPERIMENTAL_FACTORS.keys()),\n",
    "                             alternatives=ALTERNATIVES,\n",
    "                             target=TARGET,\n",
    "                             lower_is_better=False, impute_missing=False)\n",
    "\n",
    "tol = 0.2\n",
    "rf = rf.loc[:, rf.isna().sum(axis=0) <= rf.shape[0] * tol]\n",
    "rf = rf.loc[rf.isna().sum(axis=1) <= rf.shape[1] * tol, :]\n",
    "df = df.loc[df.set_index(list(EXPERIMENTAL_FACTORS.keys())).index.isin(rf.columns)]\n",
    "\n",
    "# Check whether exactly one of the experimental factors is None\n",
    "assert sum(value is None for value in EXPERIMENTAL_FACTORS.values()) == 1, \"Exactly one experimental factor must be null in config.yaml\"\n",
    "\n",
    "columns_to_check = set(EXPERIMENTAL_FACTORS.keys()).union({TARGET, ALTERNATIVES})\n",
    "# Check whether the factors listed in the config actually exist in the df\n",
    "if not_in_df := columns_to_check - set(df.columns):\n",
    "    raise ValueError(f\"The following columns are missing from the dataframe: {not_in_df}\")\n",
    "\n",
    "# Check whether the factors listed in the config are exhaustive\n",
    "if not_in_config:= set(df.columns) - columns_to_check:\n",
    "    raise ValueError(f\"The following columns in the dataframe are not required: {not_in_config}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "score_key\nexact_str_match                          97167\nmultiple_choice_grade                    58029\nrougeLsum                                16855\nnormalized_aggregate_score                4016\ndifference_score                          2131\nbleu                                      1716\naccuracy                                   608\nnumeric_match_with_0_1_relative_error      528\nf1                                         468\nmacro_f1                                   212\nalignment_score                            192\nfull                                       189\nsequence_f1                                180\nbleurt                                     180\nlog_likelihood                             176\ncustom_score                               160\ntargets_reached                            128\nfairness                                   128\naverage_log_probability                     96\nmain_words_match                            96\nbias_level                                  64\nmean_accuracy                               56\ngender_stereotype_score                     56\ngender_bias_score                           56\nlog10_p_dev                                 48\noverall                                     32\npair-wise-accuracy                          32\noverall gender bias                         32\ncreativity_and_consistency_score            32\ncorrect                                     32\nrelative_score                              32\ncombined_bias                               32\ngender_minority_stereotype_score            24\ngender_minority_bias_score                  24\navg_acc                                     20\ncorrect_prob_mass                           17\nbleurt_diff                                 17\naverage                                      8\noverall_difference                           7\noverall_alpha_avg                            7\nName: count, dtype: int64"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(DATASET).query(\"preferred_score == score_key\")[\"score_key\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "task_name\nmult_data_wrangling                        33345\nmultiemo                                   14985\nnatural_instructions                        8370\narithmetic                                  2772\nbbq_lite_json                               2565\n                                           ...  \nauto_categorization                          132\nsimple_arithmetic_multiple_targets_json      132\ncodenames                                    132\nobject_counting                              132\nsimple_arithmetic_json                        45\nName: count, Length: 155, dtype: int64"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.task_name.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a query from the experimental factors and convert the `df`. Note that here, we assume that we are using the _all key to designate that we want to use all possible values for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "# query df for the fixed factor levels in config.yaml\n",
    "try:\n",
    "    query_string = \" and \".join(f\"{factor} == '{lvl}'\" if isinstance(lvl, str) else f\"{factor} == {lvl}\"\n",
    "                                for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                                if lvl not in [None, \"_all\"])\n",
    "    df = df.query(query_string)\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "# for the not allowed-to-vary factors, get their combinations\n",
    "try:\n",
    "    groups = df.groupby([factor for factor, lvl in EXPERIMENTAL_FACTORS.items() if lvl == \"_all\"]).groups\n",
    "except ValueError:\n",
    "    groups = {\"None\": df.index}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the kernels that we defined in the config. We will later iterate over them, as to perform our analysis over all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kernels(rank_matrix):\n",
    "    kernels = {}\n",
    "    for kernel_config in config['kernels']:\n",
    "        kernel_func = getattr(ku, kernel_config['kernel'], None)\n",
    "        \n",
    "        if kernel_func:\n",
    "            delta = kernel_config['delta']  # to get epsilon\n",
    "            match kernel_config['kernel']:\n",
    "                case \"mallows_kernel\":\n",
    "                    eps = np.sqrt(2 * (1 - np.exp(-delta)))  # assumes nu = 1/binom(n, 2)\n",
    "                case \"jaccard_kernel\":\n",
    "                    eps = np.sqrt(2 * (1 - (1-delta)))\n",
    "                case \"borda_kernel\":\n",
    "                    eps = np.sqrt(2 * (1 - np.exp(-delta)))   # assumes nu = 1/n\n",
    "                case _ :\n",
    "                    raise ValueError(f\"The kernel {kernel_config['kernel']} must be either the Jaccard, Mallows, or Borda kernel.\")\n",
    "\n",
    "            for param_key, param_values in kernel_config['params'].items():\n",
    "                if isinstance(param_values, list):\n",
    "                    for value in param_values:\n",
    "                        params = {param_key: value}\n",
    "                        if param_key == 'idx':\n",
    "                            params[param_key] = rank_matrix.index.get_loc(value)\n",
    "\n",
    "                        kernel_name = f\"{kernel_config['kernel']}_{param_key}_{value}\"\n",
    "                        kernels[kernel_name] = (kernel_func, params, eps, delta)\n",
    "                else:\n",
    "                    params = {param_key: param_values}\n",
    "                    if param_key == 'idx':\n",
    "                        params[param_key] = rank_matrix.index.get_loc(param_values)\n",
    "\n",
    "                    kernel_name = f\"{kernel_config['kernel']}_{param_key}_{param_values}\"\n",
    "                    kernels[kernel_name] = (kernel_func, params, eps, delta)\n",
    "        else:\n",
    "            print(f\"Kernel function '{kernel_config['kernel']}' not found in module 'kernels'.\")\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some directories for the different experiments we run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_directory(kernel_name, factors, delta):\n",
    "    exp0_dir = OUTPUT_DIR / \"_\".join([f\"{key}={value}\" for key, value in factors.items() if value is not None])\n",
    "    exp1_dir = exp0_dir / f\"{kernel_name}\"\n",
    "    exp21_dir = exp1_dir / f\"nstar_N_ALPHA={ALPHA}_delta={delta}_ci={LR_CONFIDENCE}\"\n",
    "    exp21_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exp22_dir = exp1_dir / \"computed_generalizability\"\n",
    "    exp22_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exp23_dir = exp1_dir / \"computed_quantiles\"\n",
    "    exp23_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return exp21_dir, exp22_dir, exp23_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a few methods that will be run in a loop until our `ec_pool` is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by sampling from our `ec_pool` and converting our samples to the corresponding rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ecs(ec_pool, sample_size):\n",
    "    assert sample_size <= len(ec_pool), f\"Sample size {sample_size} is larger than |ec_pool| = {len(ec_pool)}\"\n",
    "\n",
    "    # Sample experimental conditions\n",
    "    return RNG.choice(ec_pool, sample_size, replace=False)\n",
    "\n",
    "def compute_rankings(ecs, rank_matrix):\n",
    "    rm_ = rank_matrix.loc[:, ecs]\n",
    "    na, nv = rm_.shape\n",
    "    \n",
    "    # Generate rankings from the data\n",
    "    rankings = ru.SampleAM.from_rank_function_dataframe(rm_)\n",
    "    \n",
    "    return rankings, nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get compute the variance, variance lower bound and MMDs of these rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_and_lower_bound(rankings, n, kbar, eps, kernel, kernelargs):\n",
    "    variance = ku.var(rankings, use_rv=True, kernel=kernel, **kernelargs)\n",
    "    var_lower_bound = gu.sample_mean_embedding_lowerbound(eps, n, kbar=1, v=variance)\n",
    "    return variance, var_lower_bound\n",
    "\n",
    "def calculate_mmds(rankings, nv, kernel, kernelargs):\n",
    "    mmds = {\n",
    "        n: mmd.subsample_mmd_distribution(\n",
    "            rankings, subsample_size=n, rep=100, use_rv=True, use_key=False,\n",
    "            seed=SEED, disjoint=DISJOINT, replace=REPLACE, kernel=kernel, **kernelargs\n",
    "        )\n",
    "        for n in range(2, min(nv // 2 + 1, 50))\n",
    "    }\n",
    "    return mmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create dataframes consisting of Generalizability scores and Quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generalizability_dataframe(mmds, logepss):\n",
    "    ys = {n: [mmd.generalizability(mmde, np.exp(logeps)) for logeps in logepss] for n, mmde in mmds.items()}\n",
    "    dfy = pd.DataFrame(ys, index=logepss).reset_index().melt(id_vars='index', var_name='n', value_name='generalizability')\n",
    "    dfy.rename(columns={'index': 'log(eps)'}, inplace=True)\n",
    "    dfy['n'] = dfy['n'].astype(int)\n",
    "    return dfy\n",
    "\n",
    "def create_quantiles_dataframe(mmds):\n",
    "    qs = {n: np.log(np.quantile(mmde, ALPHA)) for n, mmde in mmds.items()}\n",
    "    dfq = pd.DataFrame(list(qs.items()), columns=['n', 'log(eps)'])\n",
    "    dfq['log(n)'] = np.log(dfq['n'])\n",
    "    return dfq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the quantiles dataframe to calculate `nstar`. To this end, we fit linear regression models on subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_regression_with_cv(dfq):\n",
    "    # Extracting features and target from DataFrame\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    cv = KFold(n_splits=len(y))\n",
    "\n",
    "    residuals, linear_predictors = [], []\n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        lr = LinearRegression().fit(X[train_index], y[train_index])\n",
    "\n",
    "        predicted = lr.predict(X[test_index])\n",
    "        residuals.extend(y[test_index] - predicted)\n",
    "\n",
    "        linear_predictors.append(lr)\n",
    "\n",
    "    return linear_predictors, residuals\n",
    "\n",
    "def predict_nstar(logepss, linear_predictors, dfq, eps):\n",
    "    X = dfq[['log(eps)']].values\n",
    "    y = dfq[['log(n)']].values\n",
    "\n",
    "    ns_pred_cv = [np.exp(lr.predict(logepss.reshape(-1, 1)).reshape(-1)) for lr in linear_predictors]\n",
    "\n",
    "    ns_pred = np.exp(LinearRegression().fit(X, y).predict(logepss.reshape(-1, 1)).reshape(-1))\n",
    "\n",
    "    nstar_cv = [pred[np.argmax(logepss > np.log(eps))] for pred in ns_pred_cv if not np.all(pred == 0)]\n",
    "\n",
    "    nstar = ns_pred[np.argmax(logepss > np.log(eps))]\n",
    "    \n",
    "    nstar_lower, nstar_upper = np.quantile(nstar_cv, [CI_LOWER, CI_UPPER])\n",
    "\n",
    "    return ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot the our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_generalizability_and_quantiles(dfy, dfq, logepss, ns_pred, ns_pred_cv, nstar, nstar_upper, nstar_lower, kernel_dir, eps):\n",
    "#     # Create figure and axes\n",
    "#     fig, axes = plt.subplots(2, 1, sharex=\"all\", figsize=(10, 8))\n",
    "#\n",
    "#     # Generalizability plot\n",
    "#     ax = axes[0]\n",
    "#     sns.lineplot(data=dfy, x=\"log(eps)\", y=\"generalizability\", hue=\"n\", ax=ax, palette=palette)\n",
    "#     ax.hlines(ALPHA, ls=\"--\", xmin=np.min(logepss), xmax=np.max(logepss), color=\"black\")\n",
    "#     for n in dfq[\"n\"].unique():\n",
    "#         ax.vlines(dfq.loc[dfq.n==n, \"log(eps)\"].iloc[0], ymin=0, ymax=ALPHA, ls=\":\")\n",
    "#     sns.despine(ax=ax)\n",
    "#\n",
    "#     # Quantiles plot\n",
    "#     ax = axes[1]\n",
    "#     ymax = max(ns_pred)\n",
    "#     sns.lineplot(data=dfq, x=\"log(eps)\", y=\"n\", ax=ax, ls=\"\", marker=\"o\", hue=\"n\", legend=False)\n",
    "#     for n in dfq[\"n\"].unique():\n",
    "#         ax.vlines(dfq.loc[dfq.n==n, \"log(eps)\"].iloc[0], ymin=n, ymax=ymax, ls=\":\")\n",
    "#\n",
    "#     ax.vlines(np.log(eps), ymin=0.1, ymax=ymax, color=\"black\", ls=\"--\")\n",
    "#     sns.lineplot(x=logepss, y=ns_pred, color=\"green\", ls=\"-.\", ax=ax)\n",
    "#\n",
    "#     for it, ns_tmp in enumerate(ns_pred_cv):\n",
    "#         if np.max(ns_tmp) > 1000:\n",
    "#             continue\n",
    "#         sns.lineplot(x=logepss, y=ns_tmp, color=\"green\", ls=\"-.\", alpha=0.5, ax=ax)\n",
    "#\n",
    "#     ax.set_xlabel(r\"$\\log(\\varepsilon)$\")\n",
    "#     ax.set_ylabel(r\"$n$\")\n",
    "#\n",
    "#     # N* Lines\n",
    "#     ax.hlines(nstar, xmin=np.min(logepss), xmax=np.log(eps), ls=\"-\", color=\"red\")\n",
    "#     ax.hlines(nstar_upper, xmin=np.min(logepss), xmax=np.log(eps), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "#     ax.hlines(nstar_lower, xmin=np.min(logepss), xmax=np.log(eps), ls=\"-\", color=\"red\", alpha=0.3)\n",
    "#     ax.set_yscale(\"log\")\n",
    "#     sns.despine(ax=ax)\n",
    "#\n",
    "#     # Finalize and save\n",
    "#     fig.suptitle(f\"Generalizability for $N = {len(ecs):02d}$\\n\"\n",
    "#                  fr\"$n^* (\\alpha={ALPHA}, \\varepsilon={eps:.2f}) = {np.ceil(nstar)}$\" + \"\\n\"\n",
    "#                  f\"${LR_CONFIDENCE}$-confidence interval: $[{np.ceil(nstar_lower)}, {np.ceil(nstar_upper)}]$\")\n",
    "#     plt.tight_layout()\n",
    "#     if FORMAT == \"pdf\" or FORMAT == \"all\":\n",
    "#         plt.savefig(kernel_dir / f\"N={len(ecs):02d}.pdf\")\n",
    "#     if FORMAT == \"png\" or FORMAT == \"all\":\n",
    "#         plt.savefig(kernel_dir / f\"N={len(ecs):02d}.png\")\n",
    "#     plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Configurations:   0%|          | 0/463 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39600228ec1940f4b640efc5b47b0c29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.ioff()\n",
    "\n",
    "for fixed_levels, idxs in tqdm(list(groups.items()), position=0, desc=\"Configurations\", leave=True):\n",
    "    idf = df.loc[idxs].reset_index(drop=True)\n",
    "\n",
    "    if idf.empty:\n",
    "        continue\n",
    "\n",
    "    # fixed levels\n",
    "    factors_dict = {factor: lvl\n",
    "                    for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                    if lvl not in [None, \"_all\"]}\n",
    "    factors_dict.update({factor: idf[factor].unique()[0] for factor, lvl in EXPERIMENTAL_FACTORS.items()\n",
    "                         if lvl == \"_all\"})\n",
    "\n",
    "    # -- convert df to rank matrix\n",
    "    rank_matrix = ru.get_rankings_from_df(idf, factors=list(EXPERIMENTAL_FACTORS.keys()), \n",
    "                                            alternatives=ALTERNATIVES,\n",
    "                                            target=TARGET,\n",
    "                                            lower_is_better=False, impute_missing=True)\n",
    "    rank_matrix = rank_matrix.fillna(rank_matrix.max())\n",
    "\n",
    "    # -- get all kernels\n",
    "    kernels = init_kernels(rank_matrix)\n",
    "\n",
    "    # -- set up the ec pool\n",
    "    ec_variable = next((key for key, value in EXPERIMENTAL_FACTORS.items() if value is None), None)\n",
    "    ec_pool = idf[ec_variable].unique()\n",
    "    ecs = np.array([])\n",
    "\n",
    "    # for kernelname, (kernel, kernelargs, epsstar) in tqdm(kernels.items(), position=1, desc='Kernels', leave=False):\n",
    "    for kernelname, (kernel, kernelargs, epsstar, deltastar) in kernels.items():\n",
    "        nstar_dir, gen_dir, quant_dir = create_experiment_directory(kernelname, factors_dict, epsstar)\n",
    "        out = []\n",
    "        # for i in tqdm(range(len(ec_pool) // SAMPLE_SIZE), desc=f'Using {kernelname}', leave=False):\n",
    "        for i in range(len(ec_pool) // SAMPLE_SIZE):\n",
    "\n",
    "            if (i+1)*SAMPLE_SIZE > len(ec_pool):\n",
    "                break\n",
    "            if (i+1)*SAMPLE_SIZE > 50:\n",
    "                break\n",
    "\n",
    "            # -- Sample new rankings from ec pool\n",
    "            ecs = sample_ecs(ec_pool, (i+1)*SAMPLE_SIZE)\n",
    "            rankings, nv = compute_rankings(ecs, rank_matrix)\n",
    "\n",
    "            # -- Compute the lower bound\n",
    "            variance, var_lower_bound = compute_variance_and_lower_bound(rankings, n=len(ecs), kbar=1, eps=epsstar, kernel=kernel, kernelargs=kernelargs)\n",
    "\n",
    "            # -- We do not need to compute dfy and dfq again if we have already computed them for another alpha/epsstar\n",
    "            if f\"dfy_{len(ecs)}\" in [x.stem for x in gen_dir.glob(\"*.parquet\")] and f\"dfmmd_{len(ecs)}\" in [x.stem for x in quant_dir.glob(\"*.parquet\")]:\n",
    "                try:\n",
    "                    dfy = pd.read_parquet(gen_dir / f\"dfy_{len(ecs)}.parquet\")\n",
    "                    dfmmd = pd.read_parquet(quant_dir / f\"dfmmd_{len(ecs)}.parquet\")\n",
    "\n",
    "                    dfq = pd.DataFrame(dfmmd.groupby(\"n\")[\"eps\"].quantile(ALPHA)).reset_index()\n",
    "                    dfq[\"log(eps)\"] = np.log(dfq[\"eps\"])\n",
    "                    dfq[\"log(n)\"] = np.log(dfq[\"n\"])\n",
    "\n",
    "                    logepss = dfy[\"log(eps)\"].unique()\n",
    "                except Exception as e:\n",
    "                    print(factors_dict)\n",
    "                    raise e\n",
    "            else:\n",
    "                # -- Compute mmds\n",
    "                mmds = calculate_mmds(rankings, nv, kernel=kernel, kernelargs=kernelargs)\n",
    "                dfmmd = pd.DataFrame(mmds).melt(var_name=\"n\", value_name=\"eps\")\n",
    "\n",
    "                # -- Compute generalizability and quantiles\n",
    "\n",
    "                # - Prepare log(eps) scale\n",
    "                logepss = np.linspace(np.log(epsstar) - 0.1, np.log(max(np.quantile(mmde, ALPHA) for mmde in mmds.values())) + 0.1, 1000)\n",
    "\n",
    "                # - Dataframe for generalizability\n",
    "                dfy = create_generalizability_dataframe(mmds, logepss)\n",
    "\n",
    "                # - Dataframe for quantiles\n",
    "                dfq = create_quantiles_dataframe(mmds)\n",
    "\n",
    "            # -- Linear Regression with Cross-Validation\n",
    "            try:\n",
    "                linear_predictors, residuals = perform_linear_regression_with_cv(dfq)\n",
    "                # -- Predictions\n",
    "                ns_pred, ns_pred_cv, nstar, nstar_lower, nstar_upper = predict_nstar(logepss, linear_predictors, dfq, epsstar)\n",
    "                singular = False\n",
    "            except ValueError:\n",
    "                nstar = nstar_lower = nstar_upper = 1\n",
    "                singular = True\n",
    "\n",
    "            # -- Plotting\n",
    "            # plot_generalizability_and_quantiles(dfy, dfq, logepss, ns_pred, ns_pred_cv, nstar, nstar_upper, nstar_lower, nstar_dir, epsstar)\n",
    "\n",
    "            # -- Storing\n",
    "            result_dict = {\n",
    "                \"kernel\": kernelname,\n",
    "                \"alpha\": ALPHA,\n",
    "                \"eps\": epsstar,\n",
    "                \"delta\": deltastar,\n",
    "                \"disjoint\": DISJOINT,\n",
    "                \"replace\": REPLACE,\n",
    "                \"N\": len(ecs),\n",
    "                \"nstar\": nstar,\n",
    "                \"nstar_lower\": nstar_lower,\n",
    "                \"nstar_upper\": nstar_upper,\n",
    "                \"variance\": variance,\n",
    "                \"var_lower_bound\": var_lower_bound,\n",
    "                \"singular\": singular\n",
    "            }\n",
    "            result_dict.update(factors_dict)\n",
    "            out.append(result_dict)\n",
    "\n",
    "            dfy.to_parquet(gen_dir / f\"dfy_{len(ecs)}.parquet\")\n",
    "            # dfq.to_parquet(quant_dir / f\"dfq_{len(ecs)}_{ALPHA}.parquet\")\n",
    "            dfmmd.to_parquet(quant_dir / f\"dfmmd_{len(ecs)}.parquet\")\n",
    "\n",
    "        # if FORMAT == \"gif\" or FORMAT == \"all\":\n",
    "        #     images = [iio.imread(image) for image in glob.glob(str(nstar_dir / \"*.png\"))]\n",
    "        #     iio.mimwrite(nstar_dir / f\"nstar.gif\", images, duration=750, loop=0)\n",
    "        # -- Store nstar predictions\n",
    "        out = pd.DataFrame(out)\n",
    "        out.to_parquet(nstar_dir / \"nstar.parquet\")\n",
    "# plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, we collect the results for nstar into a single dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "df_nstars = [pd.read_parquet(x)\n",
    "             for x in tqdm(list(OUTPUT_DIR.glob(\"**/**/**/nstar.parquet\")), desc=\"Loading dataframes\")]\n",
    "df_nstar = pd.concat(df_nstars).reset_index(drop=True)\n",
    "df_nstar[\"eps\"] = df_nstar[\"eps\"].round(3)\n",
    "\n",
    "fixed_factors = [factor for factor, lvl in EXPERIMENTAL_FACTORS.items() if lvl == \"_all\"]\n",
    "maxN = df_nstar.groupby(fixed_factors)[\"N\"].max()\n",
    "df_nstar = df_nstar.join(maxN, on=fixed_factors, rsuffix=\"max\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to prepere the dataframe for plotting.\n",
    "First of all, we fix epsilon to delta=0.05 and let alpha vary.\n",
    "The corresponding rounded values for epsilon are 0.221 for the Jaccard kernel and 0.224 for the Mallows and Borda kernels.\n",
    "Second, we make sure that N is at the maximum for every combination of levels of not allowed-to-vary factors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's fix the plotting parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", palette=\"flare_r\", context=\"paper\", font=\"times new roman\")\n",
    "\n",
    "preamble = r\"\"\"\n",
    "    \\usepackage{mathptmx}\n",
    "    \\usepackage{amsmath}\n",
    "\"\"\"\n",
    "mpl.use(\"TkAgg\")\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = preamble\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "# pretty names\n",
    "pc = {\"alpha\": r\"$\\alpha^*$\", \"eps\": r\"$\\varepsilon^*$\", \"nstar\": r\"$n^*$\", \"delta\": r\"$\\delta^*$\", \"N\": r\"$N$\", \"nstar_absrel_error\": \"relative error\"}  # pretty columns\n",
    "pk = {\"borda_kernel_idx_GPT GPT-3 XL\": r\"$\\kappa_b^{\\text{GPT3}, 1/n}$\", \"mallows_kernel_nu_auto\": r\"$\\kappa_m^{1/\\binom{n}{2}}$\", \"jaccard_kernel_k_1\": r\"$\\kappa_j^{1}$\"}\n",
    "pk.update({\"borda_kernel_idx_GPT GPT-3 XL\": \"$g_1$\", \"mallows_kernel_nu_auto\": \"$g_3$\", \"jaccard_kernel_k_1\": \"$g_2$\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now add the worst case scenario theoretical upper bound prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def theoretical_nstar(alphastar, epsstar, kbar=1):\n",
    "    beta1 = -2\n",
    "    beta0 = 2*np.log(np.sqrt(2*kbar) + np.sqrt(-4*kbar * np.log(1-alphastar)))\n",
    "    return np.exp(beta0 + beta1*np.log(epsstar))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All plots together for alpha and delta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5.5, 5.5/2.5), width_ratios=(1, 1), sharey=True)\n",
    "\n",
    "# ----  ALPHA\n",
    "ax = axes[0]\n",
    "dfplot = df_nstar.loc[(df_nstar[\"delta\"] == 0.05) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "dfplot[\"nstar_th\"] = theoretical_nstar(dfplot[pc[\"alpha\"]], dfplot[pc[\"eps\"]], kbar=1)\n",
    "\n",
    "# plot\n",
    "sns.boxplot(dfplot, x=pc[\"alpha\"], y=pc[\"nstar\"], ax=ax, hue=\"kernel\", showfliers=False, palette=\"cubehelix\",\n",
    "            dodge=True, native_scale=False, fill=False, legend=False,\n",
    "            width=0.75, boxprops={\"linewidth\": 1.2}, gap=0.25)\n",
    "# ax.set(xticks=[0.7, 0.8, 0.9, 0.99])\n",
    "ax.grid(color=\"grey\", alpha=0.2)\n",
    "\n",
    "# ----  DELTA\n",
    "ax = axes[1]\n",
    "dfplot = df_nstar.loc[(df_nstar[\"alpha\"] == 0.95) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "\n",
    "# plot\n",
    "sns.boxplot(dfplot, x=pc[\"delta\"], y=pc[\"nstar\"], ax=ax, hue=\"kernel\", showfliers=False, palette=\"cubehelix\",\n",
    "            dodge=True, native_scale=False, fill=False, legend=False,\n",
    "            width=0.75, boxprops={\"linewidth\": 1.2}, gap=0.25)\n",
    "# ax.set(xticks=[0.01, 0.1, 0.2, 0.3])\n",
    "ax.grid(color=\"grey\", alpha=0.2)\n",
    "\n",
    "# nice legend\n",
    "ax.legend(*ax.get_legend_handles_labels()).get_frame().set_edgecolor(\"w\")\n",
    "\n",
    "sns.despine(right=True, top=True)\n",
    "plt.tight_layout(pad=.5)\n",
    "plt.subplots_adjust(wspace=.12)\n",
    "plt.savefig(FIGURES_DIR / \"llms_nstar_alpha_delta.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Varying $\\alpha$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "dfplot = df_nstar.loc[(df_nstar[\"delta\"] == 0.05) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "dfplot[\"nstar_th\"] = theoretical_nstar(dfplot[pc[\"alpha\"]], dfplot[pc[\"eps\"]], kbar=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "\n",
    "plt.close(\"all\")\n",
    "order = np.sort(dfplot[pc[\"alpha\"]].unique())\n",
    "\n",
    "g = sns.FacetGrid(data=dfplot, col=\"kernel\", sharey=True, aspect=1, height=5.5/3)\n",
    "# add theoretical\n",
    "# for ax, k in zip(g.axes.flat, [\"borda\", \"jaccard\", \"mallows\"]):\n",
    "#     alphas = np.linspace(dfplot[pc[\"alpha\"]].min(), dfplot[pc[\"alpha\"]].max(), 100)\n",
    "#     eps = 0.316 if k == \"jaccard\" else 0.312\n",
    "#     nstars_th = theoretical_nstar(alphas, eps)\n",
    "#     sns.lineplot(x=alphas, y=nstars_th, ax=ax, c=\"black\", linewidth=0.5)\n",
    "\n",
    "g.map(sns.boxplot, pc[\"alpha\"], pc[\"nstar\"], width=0.4, showfliers=False, native_scale=True, order=order,\n",
    "      boxprops={\"alpha\": 0.3})\n",
    "g.map(sns.swarmplot, pc[\"alpha\"], pc[\"nstar\"], native_scale=True, order=order, size=0.75)\n",
    "g.map(plt.grid, color=\"grey\", alpha=0.2)\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set(xticks=[0.7, 0.8, 0.9, 0.99])\n",
    "\n",
    "g.tight_layout(pad=0.5)\n",
    "g.savefig(FIGURES_DIR / \"llm_nstar_alpha.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Varying $\\delta$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "dfplot = df_nstar.loc[(df_nstar[\"alpha\"] == 0.95) & (df_nstar[\"N\"] == df_nstar[\"Nmax\"])]\n",
    "\n",
    "# Make dfplot pretty\n",
    "dfplot = dfplot.rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "\n",
    "plt.close(\"all\")\n",
    "order = np.sort(dfplot[pc[\"delta\"]].unique())\n",
    "\n",
    "g = sns.FacetGrid(data=dfplot, col=\"kernel\", sharey=True, aspect=1, height=5.5/3)\n",
    "\n",
    "# # add theoretical\n",
    "# for ax, k in zip(g.axes.flat, [\"borda\", \"jaccard\", \"mallows\"]):\n",
    "#     a = 0.95\n",
    "#     epss = np.linspace(dfplot[pc[\"eps\"]].min(), dfplot[pc[\"eps\"]].max(), 100)\n",
    "#     nstars_th = theoretical_nstar(a, epss)\n",
    "#     sns.lineplot(x=epss, y=nstars_th, ax=ax, c=\"black\", linewidth=0.5)\n",
    "\n",
    "g.map(sns.boxplot, pc[\"delta\"], pc[\"nstar\"], width=0.4, showfliers=False, native_scale=True, order=order,\n",
    "      boxprops={\"alpha\": 0.3})\n",
    "g.map(sns.swarmplot, pc[\"delta\"], pc[\"nstar\"], native_scale=True, order=order, size=0.75)\n",
    "g.map(plt.grid, color=\"grey\", alpha=0.2)\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "# g.set(xticks=[0.2, 0.4, 0.6, 0.8], yscale=\"log\")\n",
    "g.set(xticks=[0.01, 0.1, 0.2, 0.3])\n",
    "\n",
    "g.tight_layout(pad=0.5)\n",
    "g.savefig(FIGURES_DIR / \"llm_nstar_delta.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the weird combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_ = dfplot.copy()\n",
    "# df_ = df_.loc[df_[\"kernel\"] == pk[\"jaccard_kernel_k_1\"]]\n",
    "# df_ = df_.loc[df_[pc[\"eps\"]] == df_[pc[\"eps\"]].max()]\n",
    "#\n",
    "# df_.loc[df_[pc[\"nstar\"]] == df_[pc[\"nstar\"]].min()][[pc[\"nstar\"], \"model\", \"tuning\", \"scoring\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nstar prediction from N\n",
    "\n",
    "See how the prediction changes with N"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "true_nstar = df_nstar.loc[df_nstar[\"N\"] == df_nstar[\"Nmax\"]].drop(columns=[\"N\", \"Nmax\"])\n",
    "keys = [\"kernel\", \"alpha\", \"eps\"] + [\"task_name\", \"number_of_shots\"]\n",
    "df_ = pd.merge(df_nstar, true_nstar, left_on=keys, right_on=keys, suffixes=(\"\", \"_true\"))[keys + [\"nstar\", \"nstar_true\", \"N\", \"Nmax\"]]\n",
    "df_[\"nstar_error\"] = df_[\"nstar\"] - df_[\"nstar_true\"]\n",
    "df_[\"nstar_relative_error\"] = (df_[\"nstar\"] - df_[\"nstar_true\"]) / df_[\"nstar_true\"]\n",
    "df_[\"nstar_absolute_error\"] = np.abs(df_[\"nstar\"] - df_[\"nstar_true\"])\n",
    "df_[\"nstar_absrel_error\"] = np.abs(df_[\"nstar\"] - df_[\"nstar_true\"]) / df_[\"nstar_true\"]\n",
    "# df_ = df_.loc[df_[\"N\"] != df_[\"Nmax\"]]\n",
    "print(len(df_.groupby([\"task_name\", \"number_of_shots\"]).groups))\n",
    "df_ = df_.query(\"Nmax == 50\")\n",
    "print(len(df_.groupby([\"task_name\", \"number_of_shots\"]).groups))\n",
    "\n",
    "dfplot = df_.copy().query(\"N < Nmax\").rename(columns=pc)\n",
    "dfplot[\"kernel\"] = dfplot[\"kernel\"].map(pk)\n",
    "\n",
    "y = pc[\"nstar_absrel_error\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5.5/2, 2))\n",
    "\n",
    "sns.boxplot(dfplot, x=pc[\"N\"], y=y, showfliers=False, fliersize=0.3, hue=\"kernel\", palette=\"cubehelix\", ax=ax, legend=True, linewidth=1.2, fill=False, gap=0.2)\n",
    "\n",
    "ax.grid(color=\"grey\", alpha=.2)\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "ax.set_ylabel(\"\")\n",
    "\n",
    "\n",
    "# nice legend\n",
    "ax.legend(*ax.get_legend_handles_labels()).get_frame().set_edgecolor(\"w\")\n",
    "sns.despine()\n",
    "plt.tight_layout(pad=.5)\n",
    "\n",
    "plt.savefig(FIGURES_DIR / \"llms_nstar_absrel_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intuitive explanation of generalizability\n",
    "\n",
    "Plot generalizability as function of 2n, for some fixed epsilon.\n",
    "On the same plot, plot the average kernel within the 2n sampled experimental conditions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Special results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "                 kernel  alpha    eps  delta  disjoint  replace   N  nstar  \\\n72   jaccard_kernel_k_1   0.70  0.316   0.05      True    False  10    1.0   \n73   jaccard_kernel_k_1   0.70  0.316   0.05      True    False  20    1.0   \n74   jaccard_kernel_k_1   0.80  0.316   0.05      True    False  10    1.0   \n75   jaccard_kernel_k_1   0.80  0.316   0.05      True    False  20    1.0   \n76   jaccard_kernel_k_1   0.95  0.141   0.01      True    False  10    1.0   \n77   jaccard_kernel_k_1   0.95  0.141   0.01      True    False  20    1.0   \n78   jaccard_kernel_k_1   0.95  0.316   0.05      True    False  10    1.0   \n79   jaccard_kernel_k_1   0.95  0.316   0.05      True    False  20    1.0   \n80   jaccard_kernel_k_1   0.95  0.447   0.10      True    False  10    1.0   \n81   jaccard_kernel_k_1   0.95  0.447   0.10      True    False  20    1.0   \n82   jaccard_kernel_k_1   0.95  0.632   0.20      True    False  10    1.0   \n83   jaccard_kernel_k_1   0.95  0.632   0.20      True    False  20    1.0   \n84   jaccard_kernel_k_1   0.95  0.775   0.30      True    False  10    1.0   \n85   jaccard_kernel_k_1   0.95  0.775   0.30      True    False  20    1.0   \n86   jaccard_kernel_k_1   0.99  0.316   0.05      True    False  10    1.0   \n87   jaccard_kernel_k_1   0.99  0.316   0.05      True    False  20    1.0   \n88   jaccard_kernel_k_1   0.90  0.316   0.05      True    False  10    1.0   \n89   jaccard_kernel_k_1   0.90  0.316   0.05      True    False  20    1.0   \n126  jaccard_kernel_k_1   0.70  0.316   0.05      True    False  10    1.0   \n127  jaccard_kernel_k_1   0.70  0.316   0.05      True    False  20    1.0   \n128  jaccard_kernel_k_1   0.80  0.316   0.05      True    False  10    1.0   \n129  jaccard_kernel_k_1   0.80  0.316   0.05      True    False  20    1.0   \n130  jaccard_kernel_k_1   0.95  0.141   0.01      True    False  10    1.0   \n131  jaccard_kernel_k_1   0.95  0.141   0.01      True    False  20    1.0   \n132  jaccard_kernel_k_1   0.95  0.316   0.05      True    False  10    1.0   \n133  jaccard_kernel_k_1   0.95  0.316   0.05      True    False  20    1.0   \n134  jaccard_kernel_k_1   0.95  0.447   0.10      True    False  10    1.0   \n135  jaccard_kernel_k_1   0.95  0.447   0.10      True    False  20    1.0   \n136  jaccard_kernel_k_1   0.95  0.632   0.20      True    False  10    1.0   \n137  jaccard_kernel_k_1   0.95  0.632   0.20      True    False  20    1.0   \n138  jaccard_kernel_k_1   0.95  0.775   0.30      True    False  10    1.0   \n139  jaccard_kernel_k_1   0.95  0.775   0.30      True    False  20    1.0   \n140  jaccard_kernel_k_1   0.99  0.316   0.05      True    False  10    1.0   \n141  jaccard_kernel_k_1   0.99  0.316   0.05      True    False  20    1.0   \n142  jaccard_kernel_k_1   0.90  0.316   0.05      True    False  10    1.0   \n143  jaccard_kernel_k_1   0.90  0.316   0.05      True    False  20    1.0   \n\n     nstar_lower  nstar_upper  variance  var_lower_bound  singular  \\\n72           1.0          1.0       0.0         0.906679      True   \n73           1.0          1.0       0.0         0.991291      True   \n74           1.0          1.0       0.0         0.906679      True   \n75           1.0          1.0       0.0         0.991291      True   \n76           1.0          1.0       0.0         0.653773      True   \n77           1.0          1.0       0.0         0.880127      True   \n78           1.0          1.0       0.0         0.906679      True   \n79           1.0          1.0       0.0         0.991291      True   \n80           1.0          1.0       0.0         0.965059      True   \n81           1.0          1.0       0.0         0.998779      True   \n82           1.0          1.0       0.0         0.991291      True   \n83           1.0          1.0       0.0         0.999924      True   \n84           1.0          1.0       0.0         0.997001      True   \n85           1.0          1.0       0.0         0.999991      True   \n86           1.0          1.0       0.0         0.906679      True   \n87           1.0          1.0       0.0         0.991291      True   \n88           1.0          1.0       0.0         0.906679      True   \n89           1.0          1.0       0.0         0.991291      True   \n126          1.0          1.0       0.0         0.906679      True   \n127          1.0          1.0       0.0         0.991291      True   \n128          1.0          1.0       0.0         0.906679      True   \n129          1.0          1.0       0.0         0.991291      True   \n130          1.0          1.0       0.0         0.653773      True   \n131          1.0          1.0       0.0         0.880127      True   \n132          1.0          1.0       0.0         0.906679      True   \n133          1.0          1.0       0.0         0.991291      True   \n134          1.0          1.0       0.0         0.965059      True   \n135          1.0          1.0       0.0         0.998779      True   \n136          1.0          1.0       0.0         0.991291      True   \n137          1.0          1.0       0.0         0.999924      True   \n138          1.0          1.0       0.0         0.997001      True   \n139          1.0          1.0       0.0         0.999991      True   \n140          1.0          1.0       0.0         0.906679      True   \n141          1.0          1.0       0.0         0.991291      True   \n142          1.0          1.0       0.0         0.906679      True   \n143          1.0          1.0       0.0         0.991291      True   \n\n      task_name  number_of_shots  Nmax  \n72   arithmetic                1    20  \n73   arithmetic                1    20  \n74   arithmetic                1    20  \n75   arithmetic                1    20  \n76   arithmetic                1    20  \n77   arithmetic                1    20  \n78   arithmetic                1    20  \n79   arithmetic                1    20  \n80   arithmetic                1    20  \n81   arithmetic                1    20  \n82   arithmetic                1    20  \n83   arithmetic                1    20  \n84   arithmetic                1    20  \n85   arithmetic                1    20  \n86   arithmetic                1    20  \n87   arithmetic                1    20  \n88   arithmetic                1    20  \n89   arithmetic                1    20  \n126  arithmetic                2    20  \n127  arithmetic                2    20  \n128  arithmetic                2    20  \n129  arithmetic                2    20  \n130  arithmetic                2    20  \n131  arithmetic                2    20  \n132  arithmetic                2    20  \n133  arithmetic                2    20  \n134  arithmetic                2    20  \n135  arithmetic                2    20  \n136  arithmetic                2    20  \n137  arithmetic                2    20  \n138  arithmetic                2    20  \n139  arithmetic                2    20  \n140  arithmetic                2    20  \n141  arithmetic                2    20  \n142  arithmetic                2    20  \n143  arithmetic                2    20  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kernel</th>\n      <th>alpha</th>\n      <th>eps</th>\n      <th>delta</th>\n      <th>disjoint</th>\n      <th>replace</th>\n      <th>N</th>\n      <th>nstar</th>\n      <th>nstar_lower</th>\n      <th>nstar_upper</th>\n      <th>variance</th>\n      <th>var_lower_bound</th>\n      <th>singular</th>\n      <th>task_name</th>\n      <th>number_of_shots</th>\n      <th>Nmax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>72</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.70</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.70</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.80</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.80</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.141</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.653773</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.141</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.880127</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.447</td>\n      <td>0.10</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.965059</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.447</td>\n      <td>0.10</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.998779</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.632</td>\n      <td>0.20</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.632</td>\n      <td>0.20</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.999924</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.775</td>\n      <td>0.30</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.997001</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.775</td>\n      <td>0.30</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.999991</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.99</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.99</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.90</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.90</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.70</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.70</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.80</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.80</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.141</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.653773</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.141</td>\n      <td>0.01</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.880127</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.447</td>\n      <td>0.10</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.965059</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.447</td>\n      <td>0.10</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.998779</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.632</td>\n      <td>0.20</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.632</td>\n      <td>0.20</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.999924</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.775</td>\n      <td>0.30</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.997001</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.775</td>\n      <td>0.30</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.999991</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.99</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.99</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.90</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.906679</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.90</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nstar.query(\"nstar == 1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "task_name\nmultiemo                405\nmult_data_wrangling     405\nnatural_instructions    405\narithmetic              162\nbbq_lite_json            81\nconlang_translation      81\ngem                      81\nlinguistic_mappings      81\nName: count, dtype: int64"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nstar.task_name.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "                 kernel  alpha    eps  delta  disjoint  replace   N  \\\n79   jaccard_kernel_k_1   0.95  0.316   0.05      True    False  20   \n133  jaccard_kernel_k_1   0.95  0.316   0.05      True    False  20   \n255  jaccard_kernel_k_1   0.95  0.316   0.05      True    False  10   \n\n         nstar  nstar_lower  nstar_upper  variance  var_lower_bound  singular  \\\n79    1.000000     1.000000     1.000000  0.000000         0.991291      True   \n133   1.000000     1.000000     1.000000  0.000000         0.991291      True   \n255  43.790123    27.412227    56.877656  0.777778         0.349889     False   \n\n               task_name  number_of_shots  Nmax  \n79            arithmetic                1    20  \n133           arithmetic                2    20  \n255  conlang_translation                0    10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kernel</th>\n      <th>alpha</th>\n      <th>eps</th>\n      <th>delta</th>\n      <th>disjoint</th>\n      <th>replace</th>\n      <th>N</th>\n      <th>nstar</th>\n      <th>nstar_lower</th>\n      <th>nstar_upper</th>\n      <th>variance</th>\n      <th>var_lower_bound</th>\n      <th>singular</th>\n      <th>task_name</th>\n      <th>number_of_shots</th>\n      <th>Nmax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>79</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>1</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>20</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.991291</td>\n      <td>True</td>\n      <td>arithmetic</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>255</th>\n      <td>jaccard_kernel_k_1</td>\n      <td>0.95</td>\n      <td>0.316</td>\n      <td>0.05</td>\n      <td>True</td>\n      <td>False</td>\n      <td>10</td>\n      <td>43.790123</td>\n      <td>27.412227</td>\n      <td>56.877656</td>\n      <td>0.777778</td>\n      <td>0.349889</td>\n      <td>False</td>\n      <td>conlang_translation</td>\n      <td>0</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = \"jaccard_kernel_k_1\"\n",
    "\n",
    "df_ = df_nstar.query(\"kernel==@kernel and alpha==0.95 and delta==0.05\")\n",
    "df_ = df_.loc[df_[\"N\"] == df_[\"Nmax\"]]\n",
    "df_.loc[(df_[\"nstar\"] == df_[\"nstar\"].min()) | (df_[\"nstar\"] == df_[\"nstar\"].max())]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "                  arithmetic arithmetic:1_digit_addition  \\\n                  arithmetic                  arithmetic   \n                           1                           1   \nGPT GPT-3 Small         20.0                        16.0   \nBIG-G T=1 2b            26.0                        19.0   \nBIG-G T=1 27b           22.0                        18.0   \nBIG-G sparse 2m         42.0                        27.0   \nPaLM 8b                 13.0                        14.0   \nGPT GPT-3 200B           NaN                         NaN   \nBIG-G T=1 244m          30.0                        21.0   \nBIG-G T=0 244m          28.0                        23.0   \nBIG-G sparse 125m       33.0                        21.0   \nBIG-G sparse 422m       21.0                        20.0   \nBIG-G T=1 8b            23.0                        24.0   \nBIG-G T=1 16m           34.0                        22.0   \nBIG-G T=0 128b          14.0                         3.0   \nBIG-G T=1 125m          38.0                        21.0   \nBIG-G T=0 53m           32.0                        21.0   \nBIG-G sparse 8b          9.0                         7.0   \nBIG-G sparse 1b          4.0                        10.0   \nBIG-G sparse 2b         11.0                        10.0   \nPaLM 535b                0.0                         0.0   \nGPT GPT-3 XL             8.0                         8.0   \nBIG-G T=0 2b            17.0                        12.0   \nGPT GPT-3 13B            2.0                         2.0   \nGPT GPT-3 6B             7.0                         4.0   \nBIG-G sparse 53m        40.0                        23.0   \nBIG-G T=0 27b            6.0                        10.0   \nBIG-G T=1 422m          31.0                        20.0   \nBIG-G T=0 1b            18.0                        18.0   \nBIG-G T=0 422m          29.0                        22.0   \nBIG-G T=1 4b            25.0                        19.0   \nBIG-G T=0 4b            10.0                        17.0   \nBIG-G sparse 244m       24.0                        21.0   \nGPT GPT-3 Large         15.0                        11.0   \nBIG-G sparse 16m        37.0                        23.0   \nBIG-G T=0 16m           39.0                        23.0   \nBIG-G sparse 4b          5.0                         5.0   \nBIG-G T=1 1b            27.0                        26.0   \nBIG-G T=1 2m            41.0                        25.0   \nGPT GPT-3 Medium        16.0                        13.0   \nPaLM 64b                 1.0                         1.0   \nBIG-G T=1 53m           36.0                        21.0   \nBIG-G T=0 125m          35.0                        23.0   \nBIG-G T=0 8b            12.0                         9.0   \nBIG-G T=0 2m            43.0                        27.0   \nGPT GPT-3 3B             3.0                         6.0   \nBIG-G T=1 128b          19.0                        15.0   \n\n                  arithmetic:1_digit_division  \\\n                                   arithmetic   \n                                            1   \nGPT GPT-3 Small                           8.0   \nBIG-G T=1 2b                              8.0   \nBIG-G T=1 27b                             9.0   \nBIG-G sparse 2m                           9.0   \nPaLM 8b                                   2.0   \nGPT GPT-3 200B                            NaN   \nBIG-G T=1 244m                            9.0   \nBIG-G T=0 244m                            9.0   \nBIG-G sparse 125m                         8.0   \nBIG-G sparse 422m                         8.0   \nBIG-G T=1 8b                             11.0   \nBIG-G T=1 16m                             9.0   \nBIG-G T=0 128b                            4.0   \nBIG-G T=1 125m                            9.0   \nBIG-G T=0 53m                             7.0   \nBIG-G sparse 8b                           5.0   \nBIG-G sparse 1b                           4.0   \nBIG-G sparse 2b                           4.0   \nPaLM 535b                                 0.0   \nGPT GPT-3 XL                              4.0   \nBIG-G T=0 2b                              6.0   \nGPT GPT-3 13B                             3.0   \nGPT GPT-3 6B                              5.0   \nBIG-G sparse 53m                          8.0   \nBIG-G T=0 27b                             8.0   \nBIG-G T=1 422m                            7.0   \nBIG-G T=0 1b                              4.0   \nBIG-G T=0 422m                            8.0   \nBIG-G T=1 4b                              7.0   \nBIG-G T=0 4b                              4.0   \nBIG-G sparse 244m                         8.0   \nGPT GPT-3 Large                           6.0   \nBIG-G sparse 16m                          8.0   \nBIG-G T=0 16m                             9.0   \nBIG-G sparse 4b                           7.0   \nBIG-G T=1 1b                              4.0   \nBIG-G T=1 2m                              8.0   \nGPT GPT-3 Medium                          6.0   \nPaLM 64b                                  1.0   \nBIG-G T=1 53m                            10.0   \nBIG-G T=0 125m                            8.0   \nBIG-G T=0 8b                              4.0   \nBIG-G T=0 2m                             12.0   \nGPT GPT-3 3B                              4.0   \nBIG-G T=1 128b                            5.0   \n\n                  arithmetic:1_digit_multiplication  \\\n                                         arithmetic   \n                                                  1   \nGPT GPT-3 Small                                23.0   \nBIG-G T=1 2b                                   24.0   \nBIG-G T=1 27b                                  18.0   \nBIG-G sparse 2m                                27.0   \nPaLM 8b                                        13.0   \nGPT GPT-3 200B                                  NaN   \nBIG-G T=1 244m                                 21.0   \nBIG-G T=0 244m                                 20.0   \nBIG-G sparse 125m                              27.0   \nBIG-G sparse 422m                              24.0   \nBIG-G T=1 8b                                   20.0   \nBIG-G T=1 16m                                  27.0   \nBIG-G T=0 128b                                  2.0   \nBIG-G T=1 125m                                 27.0   \nBIG-G T=0 53m                                  25.0   \nBIG-G sparse 8b                                 7.0   \nBIG-G sparse 1b                                 9.0   \nBIG-G sparse 2b                                 9.0   \nPaLM 535b                                       0.0   \nGPT GPT-3 XL                                   11.0   \nBIG-G T=0 2b                                   16.0   \nGPT GPT-3 13B                                   5.0   \nGPT GPT-3 6B                                    8.0   \nBIG-G sparse 53m                               27.0   \nBIG-G T=0 27b                                   3.0   \nBIG-G T=1 422m                                 22.0   \nBIG-G T=0 1b                                    6.0   \nBIG-G T=0 422m                                 24.0   \nBIG-G T=1 4b                                   21.0   \nBIG-G T=0 4b                                   12.0   \nBIG-G sparse 244m                              26.0   \nGPT GPT-3 Large                                14.0   \nBIG-G sparse 16m                               27.0   \nBIG-G T=0 16m                                  26.0   \nBIG-G sparse 4b                                 3.0   \nBIG-G T=1 1b                                   19.0   \nBIG-G T=1 2m                                   27.0   \nGPT GPT-3 Medium                               15.0   \nPaLM 64b                                        1.0   \nBIG-G T=1 53m                                  27.0   \nBIG-G T=0 125m                                 27.0   \nBIG-G T=0 8b                                   17.0   \nBIG-G T=0 2m                                   28.0   \nGPT GPT-3 3B                                    4.0   \nBIG-G T=1 128b                                 10.0   \n\n                  arithmetic:1_digit_subtraction arithmetic:2_digit_addition  \\\n                                      arithmetic                  arithmetic   \n                                               1                           1   \nGPT GPT-3 Small                             18.0                        13.0   \nBIG-G T=1 2b                                19.0                        14.0   \nBIG-G T=1 27b                                7.0                        12.0   \nBIG-G sparse 2m                             30.0                        23.0   \nPaLM 8b                                     11.0                         5.0   \nGPT GPT-3 200B                               NaN                         NaN   \nBIG-G T=1 244m                              21.0                        21.0   \nBIG-G T=0 244m                              20.0                        16.0   \nBIG-G sparse 125m                           28.0                        21.0   \nBIG-G sparse 422m                           14.0                        18.0   \nBIG-G T=1 8b                                23.0                        16.0   \nBIG-G T=1 16m                               26.0                        22.0   \nBIG-G T=0 128b                               7.0                         4.0   \nBIG-G T=1 125m                              27.0                        22.0   \nBIG-G T=0 53m                               25.0                        20.0   \nBIG-G sparse 8b                              7.0                         7.0   \nBIG-G sparse 1b                              3.0                         6.0   \nBIG-G sparse 2b                             12.0                         3.0   \nPaLM 535b                                    0.0                         0.0   \nGPT GPT-3 XL                                 5.0                        10.0   \nBIG-G T=0 2b                                 9.0                         4.0   \nGPT GPT-3 13B                                5.0                         5.0   \nGPT GPT-3 6B                                 8.0                         8.0   \nBIG-G sparse 53m                            29.0                        21.0   \nBIG-G T=0 27b                                4.0                         2.0   \nBIG-G T=1 422m                              22.0                        22.0   \nBIG-G T=0 1b                                17.0                        15.0   \nBIG-G T=0 422m                              24.0                        17.0   \nBIG-G T=1 4b                                25.0                        20.0   \nBIG-G T=0 4b                                15.0                        19.0   \nBIG-G sparse 244m                           10.0                        17.0   \nGPT GPT-3 Large                             18.0                        10.0   \nBIG-G sparse 16m                            28.0                        22.0   \nBIG-G T=0 16m                               27.0                        22.0   \nBIG-G sparse 4b                              6.0                         2.0   \nBIG-G T=1 1b                                20.0                        19.0   \nBIG-G T=1 2m                                30.0                        23.0   \nGPT GPT-3 Medium                            16.0                         5.0   \nPaLM 64b                                     2.0                         1.0   \nBIG-G T=1 53m                               27.0                        23.0   \nBIG-G T=0 125m                              28.0                        22.0   \nBIG-G T=0 8b                                 7.0                        11.0   \nBIG-G T=0 2m                                29.0                        23.0   \nGPT GPT-3 3B                                 1.0                         5.0   \nBIG-G T=1 128b                              13.0                         9.0   \n\n                  arithmetic:2_digit_division  \\\n                                   arithmetic   \n                                            1   \nGPT GPT-3 Small                          18.0   \nBIG-G T=1 2b                             20.0   \nBIG-G T=1 27b                            19.0   \nBIG-G sparse 2m                          23.0   \nPaLM 8b                                   2.0   \nGPT GPT-3 200B                            NaN   \nBIG-G T=1 244m                           21.0   \nBIG-G T=0 244m                           20.0   \nBIG-G sparse 125m                        21.0   \nBIG-G sparse 422m                        15.0   \nBIG-G T=1 8b                             20.0   \nBIG-G T=1 16m                            22.0   \nBIG-G T=0 128b                            4.0   \nBIG-G T=1 125m                           22.0   \nBIG-G T=0 53m                            20.0   \nBIG-G sparse 8b                           9.0   \nBIG-G sparse 1b                          10.0   \nBIG-G sparse 2b                           7.0   \nPaLM 535b                                 0.0   \nGPT GPT-3 XL                              8.0   \nBIG-G T=0 2b                             14.0   \nGPT GPT-3 13B                             5.0   \nGPT GPT-3 6B                              3.0   \nBIG-G sparse 53m                         21.0   \nBIG-G T=0 27b                             7.0   \nBIG-G T=1 422m                           24.0   \nBIG-G T=0 1b                             14.0   \nBIG-G T=0 422m                           20.0   \nBIG-G T=1 4b                             23.0   \nBIG-G T=0 4b                             14.0   \nBIG-G sparse 244m                        19.0   \nGPT GPT-3 Large                          12.0   \nBIG-G sparse 16m                         21.0   \nBIG-G T=0 16m                            23.0   \nBIG-G sparse 4b                          10.0   \nBIG-G T=1 1b                             17.0   \nBIG-G T=1 2m                             24.0   \nGPT GPT-3 Medium                         16.0   \nPaLM 64b                                  1.0   \nBIG-G T=1 53m                            25.0   \nBIG-G T=0 125m                           21.0   \nBIG-G T=0 8b                             11.0   \nBIG-G T=0 2m                             25.0   \nGPT GPT-3 3B                              6.0   \nBIG-G T=1 128b                           13.0   \n\n                  arithmetic:2_digit_multiplication  \\\n                                         arithmetic   \n                                                  1   \nGPT GPT-3 Small                                17.0   \nBIG-G T=1 2b                                   13.0   \nBIG-G T=1 27b                                  13.0   \nBIG-G sparse 2m                                22.0   \nPaLM 8b                                        11.0   \nGPT GPT-3 200B                                  NaN   \nBIG-G T=1 244m                                 19.0   \nBIG-G T=0 244m                                 19.0   \nBIG-G sparse 125m                              22.0   \nBIG-G sparse 422m                              12.0   \nBIG-G T=1 8b                                   13.0   \nBIG-G T=1 16m                                  22.0   \nBIG-G T=0 128b                                 15.0   \nBIG-G T=1 125m                                 21.0   \nBIG-G T=0 53m                                  21.0   \nBIG-G sparse 8b                                 2.0   \nBIG-G sparse 1b                                 9.0   \nBIG-G sparse 2b                                 9.0   \nPaLM 535b                                       0.0   \nGPT GPT-3 XL                                   14.0   \nBIG-G T=0 2b                                   15.0   \nGPT GPT-3 13B                                   3.0   \nGPT GPT-3 6B                                    5.0   \nBIG-G sparse 53m                               22.0   \nBIG-G T=0 27b                                   7.0   \nBIG-G T=1 422m                                 19.0   \nBIG-G T=0 1b                                   12.0   \nBIG-G T=0 422m                                 20.0   \nBIG-G T=1 4b                                   16.0   \nBIG-G T=0 4b                                    4.0   \nBIG-G sparse 244m                              18.0   \nGPT GPT-3 Large                                 5.0   \nBIG-G sparse 16m                               22.0   \nBIG-G T=0 16m                                  22.0   \nBIG-G sparse 4b                                 6.0   \nBIG-G T=1 1b                                   15.0   \nBIG-G T=1 2m                                   22.0   \nGPT GPT-3 Medium                                8.0   \nPaLM 64b                                        1.0   \nBIG-G T=1 53m                                  22.0   \nBIG-G T=0 125m                                 22.0   \nBIG-G T=0 8b                                   13.0   \nBIG-G T=0 2m                                   22.0   \nGPT GPT-3 3B                                    5.0   \nBIG-G T=1 128b                                 10.0   \n\n                  arithmetic:2_digit_subtraction arithmetic:3_digit_addition  \\\n                                      arithmetic                  arithmetic   \n                                               1                           1   \nGPT GPT-3 Small                             10.0                        14.0   \nBIG-G T=1 2b                                21.0                        19.0   \nBIG-G T=1 27b                               16.0                        17.0   \nBIG-G sparse 2m                             27.0                        23.0   \nPaLM 8b                                     18.0                        12.0   \nGPT GPT-3 200B                               NaN                         NaN   \nBIG-G T=1 244m                              22.0                        22.0   \nBIG-G T=0 244m                              17.0                        20.0   \nBIG-G sparse 125m                           25.0                        23.0   \nBIG-G sparse 422m                           14.0                        20.0   \nBIG-G T=1 8b                                14.0                        16.0   \nBIG-G T=1 16m                               25.0                        23.0   \nBIG-G T=0 128b                               9.0                        17.0   \nBIG-G T=1 125m                              26.0                        23.0   \nBIG-G T=0 53m                               25.0                        23.0   \nBIG-G sparse 8b                             13.0                         9.0   \nBIG-G sparse 1b                              2.0                         7.0   \nBIG-G sparse 2b                             15.0                         3.0   \nPaLM 535b                                    0.0                         0.0   \nGPT GPT-3 XL                                 3.0                         6.0   \nBIG-G T=0 2b                                11.0                        15.0   \nGPT GPT-3 13B                                4.0                         4.0   \nGPT GPT-3 6B                                 6.0                        14.0   \nBIG-G sparse 53m                            26.0                        23.0   \nBIG-G T=0 27b                                1.0                         9.0   \nBIG-G T=1 422m                              23.0                        21.0   \nBIG-G T=0 1b                                19.0                         9.0   \nBIG-G T=0 422m                              21.0                        22.0   \nBIG-G T=1 4b                                24.0                        13.0   \nBIG-G T=0 4b                                13.0                         1.0   \nBIG-G sparse 244m                           12.0                        22.0   \nGPT GPT-3 Large                              7.0                        11.0   \nBIG-G sparse 16m                            26.0                        23.0   \nBIG-G T=0 16m                               26.0                        23.0   \nBIG-G sparse 4b                              5.0                         5.0   \nBIG-G T=1 1b                                23.0                        13.0   \nBIG-G T=1 2m                                27.0                        23.0   \nGPT GPT-3 Medium                            10.0                        14.0   \nPaLM 64b                                     8.0                         8.0   \nBIG-G T=1 53m                               27.0                        22.0   \nBIG-G T=0 125m                              26.0                        23.0   \nBIG-G T=0 8b                                12.0                        10.0   \nBIG-G T=0 2m                                27.0                        23.0   \nGPT GPT-3 3B                                 6.0                         2.0   \nBIG-G T=1 128b                              20.0                        18.0   \n\n                   ... arithmetic:3_digit_multiplication  \\\n                   ...                        arithmetic   \n                   ...                                 1   \nGPT GPT-3 Small    ...                              19.0   \nBIG-G T=1 2b       ...                              18.0   \nBIG-G T=1 27b      ...                              20.0   \nBIG-G sparse 2m    ...                              23.0   \nPaLM 8b            ...                              15.0   \nGPT GPT-3 200B     ...                               NaN   \nBIG-G T=1 244m     ...                              23.0   \nBIG-G T=0 244m     ...                              20.0   \nBIG-G sparse 125m  ...                              23.0   \nBIG-G sparse 422m  ...                              16.0   \nBIG-G T=1 8b       ...                              14.0   \nBIG-G T=1 16m      ...                              23.0   \nBIG-G T=0 128b     ...                              14.0   \nBIG-G T=1 125m     ...                              23.0   \nBIG-G T=0 53m      ...                              23.0   \nBIG-G sparse 8b    ...                               9.0   \nBIG-G sparse 1b    ...                               8.0   \nBIG-G sparse 2b    ...                              12.0   \nPaLM 535b          ...                               0.0   \nGPT GPT-3 XL       ...                              11.0   \nBIG-G T=0 2b       ...                               6.0   \nGPT GPT-3 13B      ...                               3.0   \nGPT GPT-3 6B       ...                              10.0   \nBIG-G sparse 53m   ...                              23.0   \nBIG-G T=0 27b      ...                               2.0   \nBIG-G T=1 422m     ...                              21.0   \nBIG-G T=0 1b       ...                              17.0   \nBIG-G T=0 422m     ...                              21.0   \nBIG-G T=1 4b       ...                              16.0   \nBIG-G T=0 4b       ...                               4.0   \nBIG-G sparse 244m  ...                              16.0   \nGPT GPT-3 Large    ...                               7.0   \nBIG-G sparse 16m   ...                              23.0   \nBIG-G T=0 16m      ...                              23.0   \nBIG-G sparse 4b    ...                               4.0   \nBIG-G T=1 1b       ...                              18.0   \nBIG-G T=1 2m       ...                              23.0   \nGPT GPT-3 Medium   ...                              11.0   \nPaLM 64b           ...                              13.0   \nBIG-G T=1 53m      ...                              23.0   \nBIG-G T=0 125m     ...                              22.0   \nBIG-G T=0 8b       ...                               5.0   \nBIG-G T=0 2m       ...                              23.0   \nGPT GPT-3 3B       ...                               1.0   \nBIG-G T=1 128b     ...                              16.0   \n\n                  arithmetic:3_digit_subtraction arithmetic:4_digit_addition  \\\n                                      arithmetic                  arithmetic   \n                                               1                           1   \nGPT GPT-3 Small                             13.0                        13.0   \nBIG-G T=1 2b                                14.0                        18.0   \nBIG-G T=1 27b                               18.0                        11.0   \nBIG-G sparse 2m                             22.0                        24.0   \nPaLM 8b                                      6.0                         3.0   \nGPT GPT-3 200B                               NaN                         NaN   \nBIG-G T=1 244m                              19.0                        21.0   \nBIG-G T=0 244m                              14.0                        14.0   \nBIG-G sparse 125m                           21.0                        23.0   \nBIG-G sparse 422m                           15.0                        19.0   \nBIG-G T=1 8b                                16.0                        15.0   \nBIG-G T=1 16m                               22.0                        24.0   \nBIG-G T=0 128b                              14.0                        17.0   \nBIG-G T=1 125m                              22.0                        24.0   \nBIG-G T=0 53m                               21.0                        24.0   \nBIG-G sparse 8b                             10.0                        14.0   \nBIG-G sparse 1b                              5.0                         7.0   \nBIG-G sparse 2b                             16.0                         1.0   \nPaLM 535b                                    0.0                         0.0   \nGPT GPT-3 XL                                 4.0                         6.0   \nBIG-G T=0 2b                                12.0                        17.0   \nGPT GPT-3 13B                                1.0                         9.0   \nGPT GPT-3 6B                                 3.0                        13.0   \nBIG-G sparse 53m                            22.0                        24.0   \nBIG-G T=0 27b                                7.0                         8.0   \nBIG-G T=1 422m                              20.0                        22.0   \nBIG-G T=0 1b                                11.0                         1.0   \nBIG-G T=0 422m                              18.0                        19.0   \nBIG-G T=1 4b                                17.0                        10.0   \nBIG-G T=0 4b                                 8.0                         2.0   \nBIG-G sparse 244m                           12.0                        15.0   \nGPT GPT-3 Large                              6.0                         9.0   \nBIG-G sparse 16m                            22.0                        24.0   \nBIG-G T=0 16m                               22.0                        24.0   \nBIG-G sparse 4b                             11.0                         1.0   \nBIG-G T=1 1b                                19.0                        12.0   \nBIG-G T=1 2m                                22.0                        24.0   \nGPT GPT-3 Medium                             9.0                        16.0   \nPaLM 64b                                     2.0                         4.0   \nBIG-G T=1 53m                               21.0                        24.0   \nBIG-G T=0 125m                              22.0                        24.0   \nBIG-G T=0 8b                                12.0                         5.0   \nBIG-G T=0 2m                                22.0                        24.0   \nGPT GPT-3 3B                                 6.0                         9.0   \nBIG-G T=1 128b                              18.0                        20.0   \n\n                  arithmetic:4_digit_division  \\\n                                   arithmetic   \n                                            1   \nGPT GPT-3 Small                          15.0   \nBIG-G T=1 2b                             14.0   \nBIG-G T=1 27b                            17.0   \nBIG-G sparse 2m                          21.0   \nPaLM 8b                                   9.0   \nGPT GPT-3 200B                            NaN   \nBIG-G T=1 244m                           18.0   \nBIG-G T=0 244m                           19.0   \nBIG-G sparse 125m                        20.0   \nBIG-G sparse 422m                        10.0   \nBIG-G T=1 8b                             11.0   \nBIG-G T=1 16m                            19.0   \nBIG-G T=0 128b                            5.0   \nBIG-G T=1 125m                           21.0   \nBIG-G T=0 53m                            21.0   \nBIG-G sparse 8b                           6.0   \nBIG-G sparse 1b                           8.0   \nBIG-G sparse 2b                           4.0   \nPaLM 535b                                 0.0   \nGPT GPT-3 XL                              7.0   \nBIG-G T=0 2b                             11.0   \nGPT GPT-3 13B                             3.0   \nGPT GPT-3 6B                              2.0   \nBIG-G sparse 53m                         21.0   \nBIG-G T=0 27b                             4.0   \nBIG-G T=1 422m                           18.0   \nBIG-G T=0 1b                             14.0   \nBIG-G T=0 422m                           20.0   \nBIG-G T=1 4b                             16.0   \nBIG-G T=0 4b                              6.0   \nBIG-G sparse 244m                        12.0   \nGPT GPT-3 Large                          13.0   \nBIG-G sparse 16m                         20.0   \nBIG-G T=0 16m                            20.0   \nBIG-G sparse 4b                          10.0   \nBIG-G T=1 1b                             17.0   \nBIG-G T=1 2m                             18.0   \nGPT GPT-3 Medium                          7.0   \nPaLM 64b                                  1.0   \nBIG-G T=1 53m                            16.0   \nBIG-G T=0 125m                           21.0   \nBIG-G T=0 8b                              5.0   \nBIG-G T=0 2m                             21.0   \nGPT GPT-3 3B                              9.0   \nBIG-G T=1 128b                           11.0   \n\n                  arithmetic:4_digit_multiplication  \\\n                                         arithmetic   \n                                                  1   \nGPT GPT-3 Small                                19.0   \nBIG-G T=1 2b                                   17.0   \nBIG-G T=1 27b                                  16.0   \nBIG-G sparse 2m                                24.0   \nPaLM 8b                                         8.0   \nGPT GPT-3 200B                                  NaN   \nBIG-G T=1 244m                                 24.0   \nBIG-G T=0 244m                                 20.0   \nBIG-G sparse 125m                              23.0   \nBIG-G sparse 422m                              17.0   \nBIG-G T=1 8b                                   10.0   \nBIG-G T=1 16m                                  24.0   \nBIG-G T=0 128b                                 18.0   \nBIG-G T=1 125m                                 24.0   \nBIG-G T=0 53m                                  23.0   \nBIG-G sparse 8b                                 6.0   \nBIG-G sparse 1b                                 2.0   \nBIG-G sparse 2b                                 4.0   \nPaLM 535b                                       0.0   \nGPT GPT-3 XL                                   13.0   \nBIG-G T=0 2b                                   10.0   \nGPT GPT-3 13B                                   9.0   \nGPT GPT-3 6B                                   11.0   \nBIG-G sparse 53m                               24.0   \nBIG-G T=0 27b                                  12.0   \nBIG-G T=1 422m                                 22.0   \nBIG-G T=0 1b                                   10.0   \nBIG-G T=0 422m                                 22.0   \nBIG-G T=1 4b                                   12.0   \nBIG-G T=0 4b                                    3.0   \nBIG-G sparse 244m                              21.0   \nGPT GPT-3 Large                                15.0   \nBIG-G sparse 16m                               24.0   \nBIG-G T=0 16m                                  24.0   \nBIG-G sparse 4b                                 3.0   \nBIG-G T=1 1b                                   21.0   \nBIG-G T=1 2m                                   24.0   \nGPT GPT-3 Medium                                5.0   \nPaLM 64b                                        8.0   \nBIG-G T=1 53m                                  24.0   \nBIG-G T=0 125m                                 24.0   \nBIG-G T=0 8b                                    1.0   \nBIG-G T=0 2m                                   24.0   \nGPT GPT-3 3B                                    7.0   \nBIG-G T=1 128b                                 14.0   \n\n                  arithmetic:4_digit_subtraction arithmetic:5_digit_addition  \\\n                                      arithmetic                  arithmetic   \n                                               1                           1   \nGPT GPT-3 Small                             17.0                        12.0   \nBIG-G T=1 2b                                22.0                        16.0   \nBIG-G T=1 27b                               16.0                        15.0   \nBIG-G sparse 2m                             27.0                        21.0   \nPaLM 8b                                     18.0                         8.0   \nGPT GPT-3 200B                               NaN                         NaN   \nBIG-G T=1 244m                              25.0                        21.0   \nBIG-G T=0 244m                              21.0                        19.0   \nBIG-G sparse 125m                           25.0                        21.0   \nBIG-G sparse 422m                           13.0                        18.0   \nBIG-G T=1 8b                                20.0                        14.0   \nBIG-G T=1 16m                               27.0                        21.0   \nBIG-G T=0 128b                               2.0                        10.0   \nBIG-G T=1 125m                              27.0                        21.0   \nBIG-G T=0 53m                               27.0                        21.0   \nBIG-G sparse 8b                              4.0                        10.0   \nBIG-G sparse 1b                              1.0                         3.0   \nBIG-G sparse 2b                             19.0                         6.0   \nPaLM 535b                                    0.0                         0.0   \nGPT GPT-3 XL                                 5.0                         1.0   \nBIG-G T=0 2b                                15.0                        14.0   \nGPT GPT-3 13B                                3.0                         2.0   \nGPT GPT-3 6B                                 5.0                         4.0   \nBIG-G sparse 53m                            27.0                        21.0   \nBIG-G T=0 27b                                4.0                        11.0   \nBIG-G T=1 422m                              25.0                        19.0   \nBIG-G T=0 1b                                14.0                         6.0   \nBIG-G T=0 422m                              24.0                        21.0   \nBIG-G T=1 4b                                23.0                        13.0   \nBIG-G T=0 4b                                 8.0                        10.0   \nBIG-G sparse 244m                           11.0                        17.0   \nGPT GPT-3 Large                             12.0                         7.0   \nBIG-G sparse 16m                            27.0                        21.0   \nBIG-G T=0 16m                               27.0                        21.0   \nBIG-G sparse 4b                              6.0                         5.0   \nBIG-G T=1 1b                                22.0                        16.0   \nBIG-G T=1 2m                                27.0                        21.0   \nGPT GPT-3 Medium                            10.0                         8.0   \nPaLM 64b                                     7.0                         9.0   \nBIG-G T=1 53m                               26.0                        20.0   \nBIG-G T=0 125m                              26.0                        21.0   \nBIG-G T=0 8b                                 9.0                        14.0   \nBIG-G T=0 2m                                27.0                        21.0   \nGPT GPT-3 3B                                 3.0                         2.0   \nBIG-G T=1 128b                              22.0                        13.0   \n\n                  arithmetic:5_digit_division  \\\n                                   arithmetic   \n                                            1   \nGPT GPT-3 Small                          14.0   \nBIG-G T=1 2b                             18.0   \nBIG-G T=1 27b                            20.0   \nBIG-G sparse 2m                          23.0   \nPaLM 8b                                   4.0   \nGPT GPT-3 200B                            NaN   \nBIG-G T=1 244m                           21.0   \nBIG-G T=0 244m                           15.0   \nBIG-G sparse 125m                        22.0   \nBIG-G sparse 422m                        12.0   \nBIG-G T=1 8b                             12.0   \nBIG-G T=1 16m                            21.0   \nBIG-G T=0 128b                           18.0   \nBIG-G T=1 125m                           23.0   \nBIG-G T=0 53m                            22.0   \nBIG-G sparse 8b                           7.0   \nBIG-G sparse 1b                           7.0   \nBIG-G sparse 2b                          10.0   \nPaLM 535b                                 0.0   \nGPT GPT-3 XL                              9.0   \nBIG-G T=0 2b                             12.0   \nGPT GPT-3 13B                             2.0   \nGPT GPT-3 6B                             11.0   \nBIG-G sparse 53m                         23.0   \nBIG-G T=0 27b                             5.0   \nBIG-G T=1 422m                           22.0   \nBIG-G T=0 1b                             17.0   \nBIG-G T=0 422m                           19.0   \nBIG-G T=1 4b                             12.0   \nBIG-G T=0 4b                              3.0   \nBIG-G sparse 244m                        19.0   \nGPT GPT-3 Large                          13.0   \nBIG-G sparse 16m                         22.0   \nBIG-G T=0 16m                            23.0   \nBIG-G sparse 4b                           8.0   \nBIG-G T=1 1b                             17.0   \nBIG-G T=1 2m                             23.0   \nGPT GPT-3 Medium                         16.0   \nPaLM 64b                                  1.0   \nBIG-G T=1 53m                            22.0   \nBIG-G T=0 125m                           22.0   \nBIG-G T=0 8b                              5.0   \nBIG-G T=0 2m                             23.0   \nGPT GPT-3 3B                              6.0   \nBIG-G T=1 128b                           19.0   \n\n                  arithmetic:5_digit_multiplication  \\\n                                         arithmetic   \n                                                  1   \nGPT GPT-3 Small                                14.0   \nBIG-G T=1 2b                                   13.0   \nBIG-G T=1 27b                                  10.0   \nBIG-G sparse 2m                                17.0   \nPaLM 8b                                         8.0   \nGPT GPT-3 200B                                  NaN   \nBIG-G T=1 244m                                 15.0   \nBIG-G T=0 244m                                 15.0   \nBIG-G sparse 125m                              16.0   \nBIG-G sparse 422m                              12.0   \nBIG-G T=1 8b                                   12.0   \nBIG-G T=1 16m                                  17.0   \nBIG-G T=0 128b                                  3.0   \nBIG-G T=1 125m                                 17.0   \nBIG-G T=0 53m                                  17.0   \nBIG-G sparse 8b                                 3.0   \nBIG-G sparse 1b                                 5.0   \nBIG-G sparse 2b                                 1.0   \nPaLM 535b                                       0.0   \nGPT GPT-3 XL                                    6.0   \nBIG-G T=0 2b                                   12.0   \nGPT GPT-3 13B                                   8.0   \nGPT GPT-3 6B                                    4.0   \nBIG-G sparse 53m                               17.0   \nBIG-G T=0 27b                                  12.0   \nBIG-G T=1 422m                                 17.0   \nBIG-G T=0 1b                                    7.0   \nBIG-G T=0 422m                                 13.0   \nBIG-G T=1 4b                                   13.0   \nBIG-G T=0 4b                                    9.0   \nBIG-G sparse 244m                              16.0   \nGPT GPT-3 Large                                11.0   \nBIG-G sparse 16m                               17.0   \nBIG-G T=0 16m                                  17.0   \nBIG-G sparse 4b                                 9.0   \nBIG-G T=1 1b                                    7.0   \nBIG-G T=1 2m                                   17.0   \nGPT GPT-3 Medium                               11.0   \nPaLM 64b                                        2.0   \nBIG-G T=1 53m                                  17.0   \nBIG-G T=0 125m                                 17.0   \nBIG-G T=0 8b                                    7.0   \nBIG-G T=0 2m                                   17.0   \nGPT GPT-3 3B                                    2.0   \nBIG-G T=1 128b                                  3.0   \n\n                  arithmetic:5_digit_subtraction  \n                                      arithmetic  \n                                               1  \nGPT GPT-3 Small                              9.0  \nBIG-G T=1 2b                                20.0  \nBIG-G T=1 27b                               16.0  \nBIG-G sparse 2m                             26.0  \nPaLM 8b                                     14.0  \nGPT GPT-3 200B                               NaN  \nBIG-G T=1 244m                              24.0  \nBIG-G T=0 244m                              22.0  \nBIG-G sparse 125m                           26.0  \nBIG-G sparse 422m                           15.0  \nBIG-G T=1 8b                                18.0  \nBIG-G T=1 16m                               26.0  \nBIG-G T=0 128b                               8.0  \nBIG-G T=1 125m                              26.0  \nBIG-G T=0 53m                               24.0  \nBIG-G sparse 8b                             11.0  \nBIG-G sparse 1b                              4.0  \nBIG-G sparse 2b                             18.0  \nPaLM 535b                                    0.0  \nGPT GPT-3 XL                                 1.0  \nBIG-G T=0 2b                                16.0  \nGPT GPT-3 13B                                2.0  \nGPT GPT-3 6B                                 1.0  \nBIG-G sparse 53m                            26.0  \nBIG-G T=0 27b                                6.0  \nBIG-G T=1 422m                              25.0  \nBIG-G T=0 1b                                19.0  \nBIG-G T=0 422m                              23.0  \nBIG-G T=1 4b                                11.0  \nBIG-G T=0 4b                                 5.0  \nBIG-G sparse 244m                           10.0  \nGPT GPT-3 Large                              7.0  \nBIG-G sparse 16m                            26.0  \nBIG-G T=0 16m                               25.0  \nBIG-G sparse 4b                             12.0  \nBIG-G T=1 1b                                21.0  \nBIG-G T=1 2m                                26.0  \nGPT GPT-3 Medium                             9.0  \nPaLM 64b                                    13.0  \nBIG-G T=1 53m                               26.0  \nBIG-G T=0 125m                              26.0  \nBIG-G T=0 8b                                17.0  \nBIG-G T=0 2m                                26.0  \nGPT GPT-3 3B                                 3.0  \nBIG-G T=1 128b                              15.0  \n\n[45 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>arithmetic</th>\n      <th>arithmetic:1_digit_addition</th>\n      <th>arithmetic:1_digit_division</th>\n      <th>arithmetic:1_digit_multiplication</th>\n      <th>arithmetic:1_digit_subtraction</th>\n      <th>arithmetic:2_digit_addition</th>\n      <th>arithmetic:2_digit_division</th>\n      <th>arithmetic:2_digit_multiplication</th>\n      <th>arithmetic:2_digit_subtraction</th>\n      <th>arithmetic:3_digit_addition</th>\n      <th>...</th>\n      <th>arithmetic:3_digit_multiplication</th>\n      <th>arithmetic:3_digit_subtraction</th>\n      <th>arithmetic:4_digit_addition</th>\n      <th>arithmetic:4_digit_division</th>\n      <th>arithmetic:4_digit_multiplication</th>\n      <th>arithmetic:4_digit_subtraction</th>\n      <th>arithmetic:5_digit_addition</th>\n      <th>arithmetic:5_digit_division</th>\n      <th>arithmetic:5_digit_multiplication</th>\n      <th>arithmetic:5_digit_subtraction</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>...</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n      <th>arithmetic</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>...</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>GPT GPT-3 Small</th>\n      <td>20.0</td>\n      <td>16.0</td>\n      <td>8.0</td>\n      <td>23.0</td>\n      <td>18.0</td>\n      <td>13.0</td>\n      <td>18.0</td>\n      <td>17.0</td>\n      <td>10.0</td>\n      <td>14.0</td>\n      <td>...</td>\n      <td>19.0</td>\n      <td>13.0</td>\n      <td>13.0</td>\n      <td>15.0</td>\n      <td>19.0</td>\n      <td>17.0</td>\n      <td>12.0</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 2b</th>\n      <td>26.0</td>\n      <td>19.0</td>\n      <td>8.0</td>\n      <td>24.0</td>\n      <td>19.0</td>\n      <td>14.0</td>\n      <td>20.0</td>\n      <td>13.0</td>\n      <td>21.0</td>\n      <td>19.0</td>\n      <td>...</td>\n      <td>18.0</td>\n      <td>14.0</td>\n      <td>18.0</td>\n      <td>14.0</td>\n      <td>17.0</td>\n      <td>22.0</td>\n      <td>16.0</td>\n      <td>18.0</td>\n      <td>13.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 27b</th>\n      <td>22.0</td>\n      <td>18.0</td>\n      <td>9.0</td>\n      <td>18.0</td>\n      <td>7.0</td>\n      <td>12.0</td>\n      <td>19.0</td>\n      <td>13.0</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>...</td>\n      <td>20.0</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>17.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>20.0</td>\n      <td>10.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 2m</th>\n      <td>42.0</td>\n      <td>27.0</td>\n      <td>9.0</td>\n      <td>27.0</td>\n      <td>30.0</td>\n      <td>23.0</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>27.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>PaLM 8b</th>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>2.0</td>\n      <td>13.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>18.0</td>\n      <td>12.0</td>\n      <td>...</td>\n      <td>15.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>9.0</td>\n      <td>8.0</td>\n      <td>18.0</td>\n      <td>8.0</td>\n      <td>4.0</td>\n      <td>8.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 200B</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 244m</th>\n      <td>30.0</td>\n      <td>21.0</td>\n      <td>9.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>19.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>19.0</td>\n      <td>21.0</td>\n      <td>18.0</td>\n      <td>24.0</td>\n      <td>25.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>15.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 244m</th>\n      <td>28.0</td>\n      <td>23.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>16.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>17.0</td>\n      <td>20.0</td>\n      <td>...</td>\n      <td>20.0</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>21.0</td>\n      <td>19.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 125m</th>\n      <td>33.0</td>\n      <td>21.0</td>\n      <td>8.0</td>\n      <td>27.0</td>\n      <td>28.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>25.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>20.0</td>\n      <td>23.0</td>\n      <td>25.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>16.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 422m</th>\n      <td>21.0</td>\n      <td>20.0</td>\n      <td>8.0</td>\n      <td>24.0</td>\n      <td>14.0</td>\n      <td>18.0</td>\n      <td>15.0</td>\n      <td>12.0</td>\n      <td>14.0</td>\n      <td>20.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>19.0</td>\n      <td>10.0</td>\n      <td>17.0</td>\n      <td>13.0</td>\n      <td>18.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 8b</th>\n      <td>23.0</td>\n      <td>24.0</td>\n      <td>11.0</td>\n      <td>20.0</td>\n      <td>23.0</td>\n      <td>16.0</td>\n      <td>20.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>16.0</td>\n      <td>...</td>\n      <td>14.0</td>\n      <td>16.0</td>\n      <td>15.0</td>\n      <td>11.0</td>\n      <td>10.0</td>\n      <td>20.0</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 16m</th>\n      <td>34.0</td>\n      <td>22.0</td>\n      <td>9.0</td>\n      <td>27.0</td>\n      <td>26.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>25.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>19.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 128b</th>\n      <td>14.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>17.0</td>\n      <td>...</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>17.0</td>\n      <td>5.0</td>\n      <td>18.0</td>\n      <td>2.0</td>\n      <td>10.0</td>\n      <td>18.0</td>\n      <td>3.0</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 125m</th>\n      <td>38.0</td>\n      <td>21.0</td>\n      <td>9.0</td>\n      <td>27.0</td>\n      <td>27.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>21.0</td>\n      <td>26.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 53m</th>\n      <td>32.0</td>\n      <td>21.0</td>\n      <td>7.0</td>\n      <td>25.0</td>\n      <td>25.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>21.0</td>\n      <td>25.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>17.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 8b</th>\n      <td>9.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>2.0</td>\n      <td>13.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>14.0</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>10.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 1b</th>\n      <td>4.0</td>\n      <td>10.0</td>\n      <td>4.0</td>\n      <td>9.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>9.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>...</td>\n      <td>8.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 2b</th>\n      <td>11.0</td>\n      <td>10.0</td>\n      <td>4.0</td>\n      <td>9.0</td>\n      <td>12.0</td>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>15.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>12.0</td>\n      <td>16.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>19.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>PaLM 535b</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 XL</th>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>4.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>8.0</td>\n      <td>14.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>11.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>13.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 2b</th>\n      <td>17.0</td>\n      <td>12.0</td>\n      <td>6.0</td>\n      <td>16.0</td>\n      <td>9.0</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>11.0</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>12.0</td>\n      <td>17.0</td>\n      <td>11.0</td>\n      <td>10.0</td>\n      <td>15.0</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 13B</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>3.0</td>\n      <td>9.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 6B</th>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>14.0</td>\n      <td>...</td>\n      <td>10.0</td>\n      <td>3.0</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>11.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 53m</th>\n      <td>40.0</td>\n      <td>23.0</td>\n      <td>8.0</td>\n      <td>27.0</td>\n      <td>29.0</td>\n      <td>21.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 27b</th>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>8.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>4.0</td>\n      <td>12.0</td>\n      <td>4.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>12.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 422m</th>\n      <td>31.0</td>\n      <td>20.0</td>\n      <td>7.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>19.0</td>\n      <td>23.0</td>\n      <td>21.0</td>\n      <td>...</td>\n      <td>21.0</td>\n      <td>20.0</td>\n      <td>22.0</td>\n      <td>18.0</td>\n      <td>22.0</td>\n      <td>25.0</td>\n      <td>19.0</td>\n      <td>22.0</td>\n      <td>17.0</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 1b</th>\n      <td>18.0</td>\n      <td>18.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>17.0</td>\n      <td>15.0</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>19.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>17.0</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>14.0</td>\n      <td>6.0</td>\n      <td>17.0</td>\n      <td>7.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 422m</th>\n      <td>29.0</td>\n      <td>22.0</td>\n      <td>8.0</td>\n      <td>24.0</td>\n      <td>24.0</td>\n      <td>17.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>21.0</td>\n      <td>18.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>19.0</td>\n      <td>13.0</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 4b</th>\n      <td>25.0</td>\n      <td>19.0</td>\n      <td>7.0</td>\n      <td>21.0</td>\n      <td>25.0</td>\n      <td>20.0</td>\n      <td>23.0</td>\n      <td>16.0</td>\n      <td>24.0</td>\n      <td>13.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>10.0</td>\n      <td>16.0</td>\n      <td>12.0</td>\n      <td>23.0</td>\n      <td>13.0</td>\n      <td>12.0</td>\n      <td>13.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 4b</th>\n      <td>10.0</td>\n      <td>17.0</td>\n      <td>4.0</td>\n      <td>12.0</td>\n      <td>15.0</td>\n      <td>19.0</td>\n      <td>14.0</td>\n      <td>4.0</td>\n      <td>13.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>3.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 244m</th>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>8.0</td>\n      <td>26.0</td>\n      <td>10.0</td>\n      <td>17.0</td>\n      <td>19.0</td>\n      <td>18.0</td>\n      <td>12.0</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>12.0</td>\n      <td>15.0</td>\n      <td>12.0</td>\n      <td>21.0</td>\n      <td>11.0</td>\n      <td>17.0</td>\n      <td>19.0</td>\n      <td>16.0</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 Large</th>\n      <td>15.0</td>\n      <td>11.0</td>\n      <td>6.0</td>\n      <td>14.0</td>\n      <td>18.0</td>\n      <td>10.0</td>\n      <td>12.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>11.0</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>9.0</td>\n      <td>13.0</td>\n      <td>15.0</td>\n      <td>12.0</td>\n      <td>7.0</td>\n      <td>13.0</td>\n      <td>11.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 16m</th>\n      <td>37.0</td>\n      <td>23.0</td>\n      <td>8.0</td>\n      <td>27.0</td>\n      <td>28.0</td>\n      <td>22.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>20.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 16m</th>\n      <td>39.0</td>\n      <td>23.0</td>\n      <td>9.0</td>\n      <td>26.0</td>\n      <td>27.0</td>\n      <td>22.0</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>20.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>17.0</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G sparse 4b</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 1b</th>\n      <td>27.0</td>\n      <td>26.0</td>\n      <td>4.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>17.0</td>\n      <td>15.0</td>\n      <td>23.0</td>\n      <td>13.0</td>\n      <td>...</td>\n      <td>18.0</td>\n      <td>19.0</td>\n      <td>12.0</td>\n      <td>17.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>7.0</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 2m</th>\n      <td>41.0</td>\n      <td>25.0</td>\n      <td>8.0</td>\n      <td>27.0</td>\n      <td>30.0</td>\n      <td>23.0</td>\n      <td>24.0</td>\n      <td>22.0</td>\n      <td>27.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>18.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 Medium</th>\n      <td>16.0</td>\n      <td>13.0</td>\n      <td>6.0</td>\n      <td>15.0</td>\n      <td>16.0</td>\n      <td>5.0</td>\n      <td>16.0</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>14.0</td>\n      <td>...</td>\n      <td>11.0</td>\n      <td>9.0</td>\n      <td>16.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>8.0</td>\n      <td>16.0</td>\n      <td>11.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>PaLM 64b</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>...</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 53m</th>\n      <td>36.0</td>\n      <td>21.0</td>\n      <td>10.0</td>\n      <td>27.0</td>\n      <td>27.0</td>\n      <td>23.0</td>\n      <td>25.0</td>\n      <td>22.0</td>\n      <td>27.0</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>16.0</td>\n      <td>24.0</td>\n      <td>26.0</td>\n      <td>20.0</td>\n      <td>22.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 125m</th>\n      <td>35.0</td>\n      <td>23.0</td>\n      <td>8.0</td>\n      <td>27.0</td>\n      <td>28.0</td>\n      <td>22.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>22.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>26.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 8b</th>\n      <td>12.0</td>\n      <td>9.0</td>\n      <td>4.0</td>\n      <td>17.0</td>\n      <td>7.0</td>\n      <td>11.0</td>\n      <td>11.0</td>\n      <td>13.0</td>\n      <td>12.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>12.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>14.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=0 2m</th>\n      <td>43.0</td>\n      <td>27.0</td>\n      <td>12.0</td>\n      <td>28.0</td>\n      <td>29.0</td>\n      <td>23.0</td>\n      <td>25.0</td>\n      <td>22.0</td>\n      <td>27.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>23.0</td>\n      <td>22.0</td>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>21.0</td>\n      <td>23.0</td>\n      <td>17.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>GPT GPT-3 3B</th>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>BIG-G T=1 128b</th>\n      <td>19.0</td>\n      <td>15.0</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>13.0</td>\n      <td>9.0</td>\n      <td>13.0</td>\n      <td>10.0</td>\n      <td>20.0</td>\n      <td>18.0</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>18.0</td>\n      <td>20.0</td>\n      <td>11.0</td>\n      <td>14.0</td>\n      <td>22.0</td>\n      <td>13.0</td>\n      <td>19.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>45 rows  21 columns</p>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = ru.get_rankings_from_df(df.reset_index(drop=True),\n",
    "                             factors=list(EXPERIMENTAL_FACTORS.keys()),\n",
    "                             alternatives=ALTERNATIVES,\n",
    "                             target=TARGET,\n",
    "                             lower_is_better=False, impute_missing=False)\n",
    "\n",
    "tol = 0.2\n",
    "rf = rf.loc[:, rf.isna().sum(axis=0) <= rf.shape[0] * tol]\n",
    "rf = rf.loc[rf.isna().sum(axis=1) <= rf.shape[1] * tol, :]\n",
    "rf.loc(axis=1)[:, \"arithmetic\", 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
